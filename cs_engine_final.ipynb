{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maintenance Report NLP Analysis\n",
    "\n",
    "This notebook implements an NLP pipeline for analyzing maintenance reports using BERT and NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForTokenClassification, pipeline, logging\n",
    "from spellchecker import SpellChecker\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Suppress warnings from the transformers library\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "\n",
    "try:\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "except LookupError:\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if GPU is available\n",
    "# if platform.system() == 'Darwin':  # macOS\n",
    "#     if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "#         device = torch.device(\"mps\")\n",
    "#         print(\"MPS backend is available. PyTorch is using the GPU.\")\n",
    "#     else:\n",
    "#         device = torch.device(\"cpu\")\n",
    "#         print(\"MPS backend is not available. PyTorch is using the CPU.\")\n",
    "# else:  # Windows or other platforms\n",
    "#     if torch.cuda.is_available():\n",
    "#         device = torch.device(\"cuda\")\n",
    "#         print(\"CUDA backend is available. PyTorch is using the GPU.\")\n",
    "#     else:\n",
    "#         device = torch.device(\"cpu\")\n",
    "#         print(\"CUDA backend is not available. PyTorch is using the CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading data from cache: data/ice_makers.csv\n",
      "INFO:__main__:DataFrame shape: (296547, 5)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "from contextlib import contextmanager\n",
    "import logging\n",
    "from typing import Optional, List, Tuple\n",
    "import time\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Constants\n",
    "CSV_FILE_PATH = 'data/ice_makers.csv'\n",
    "DB_CONFIG = {\n",
    "    'driver': 'ODBC Driver 17 for SQL Server',\n",
    "    'server': '35.184.99.218',\n",
    "    'database': 'coolsys',\n",
    "    'uid': 'sqlserver',\n",
    "    'pwd': 'Ybz8Vq+|>\\H/<2py'\n",
    "}\n",
    "\n",
    "SQL_QUERY = \"\"\"\n",
    "SELECT\n",
    "    w.wrkordr_wrk_rqstd,\n",
    "    w.wrkordr_wrk_prfrmd,\n",
    "    w2.wrkordreqpmnt_wrk_rqstd,\n",
    "    w2.wrkordreqpmnt_wrk_prfrmd,\n",
    "    w3.wrkordrinvntry_dscrptn\n",
    "FROM\n",
    "    coolsys.dbo.wrkordr w\n",
    "    INNER JOIN coolsys.dbo.wrkordrinvntry w3 ON w.wrkordr_rn = w3.wrkordr_rn\n",
    "    INNER JOIN coolsys.dbo.wrkordreqpmnt w2 ON w.wrkordr_rn = w2.wrkordr_rn\n",
    "WHERE\n",
    "    w.wrkordr_wrk_rqstd LIKE '%ICE MACHINE%'\n",
    "    OR w.wrkordr_wrk_prfrmd LIKE '%ICE MACHINE%'\n",
    "    OR w2.wrkordreqpmnt_wrk_rqstd LIKE '%ICE MACHINE%'\n",
    "    OR w2.wrkordreqpmnt_wrk_prfrmd LIKE '%ICE MACHINE%'\n",
    "    OR w3.wrkordrinvntry_dscrptn LIKE '%ICE MACHINE%';\n",
    "\"\"\"\n",
    "\n",
    "@contextmanager\n",
    "def database_connection():\n",
    "    \"\"\"Context manager for database connections with proper error handling.\"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn_str = (\n",
    "            f'DRIVER={{{DB_CONFIG[\"driver\"]}}};'\n",
    "            f'SERVER={DB_CONFIG[\"server\"]};'\n",
    "            f'DATABASE={DB_CONFIG[\"database\"]};'\n",
    "            f'UID={DB_CONFIG[\"uid\"]};'\n",
    "            f'PWD={DB_CONFIG[\"pwd\"]}'\n",
    "        )\n",
    "        conn = pyodbc.connect(conn_str, timeout=30)  # Add connection timeout\n",
    "        logger.info(\"Database connection established\")\n",
    "        yield conn\n",
    "    except pyodbc.Error as e:\n",
    "        logger.error(f\"Database connection error: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "            logger.info(\"Database connection closed\")\n",
    "\n",
    "def fetch_data_from_db() -> Tuple[Optional[pd.DataFrame], Optional[str]]:\n",
    "    \"\"\"Fetch data from database with optimized performance and error handling.\"\"\"\n",
    "    try:\n",
    "        with database_connection() as conn:\n",
    "            # Configure connection for better performance\n",
    "            conn.setdecoding(pyodbc.SQL_CHAR, encoding='utf-8')\n",
    "            conn.setdecoding(pyodbc.SQL_WCHAR, encoding='utf-8')\n",
    "            conn.setencoding(encoding='utf-8')\n",
    "            \n",
    "            # Use pandas read_sql for better performance\n",
    "            start_time = time.time()\n",
    "            df = pd.read_sql(SQL_QUERY, conn)\n",
    "            logger.info(f\"Query executed in {time.time() - start_time:.2f} seconds\")\n",
    "            \n",
    "            return df, None\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error fetching data: {str(e)}\"\n",
    "        logger.error(error_msg, exc_info=True)\n",
    "        return None, error_msg\n",
    "\n",
    "def save_to_csv(df: pd.DataFrame, filepath: str) -> None:\n",
    "    \"\"\"Save DataFrame to CSV with error handling.\"\"\"\n",
    "    try:\n",
    "        # Ensure directory exists\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        \n",
    "        # Save with optimized settings\n",
    "        df.to_csv(filepath, index=False, compression='infer')\n",
    "        logger.info(f\"Data saved to {filepath}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving to CSV: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "def load_data() -> pd.DataFrame:\n",
    "    \"\"\"Main function to load data with caching mechanism.\"\"\"\n",
    "    try:\n",
    "        # Try loading from cache first\n",
    "        if os.path.exists(CSV_FILE_PATH):\n",
    "            logger.info(f\"Loading data from cache: {CSV_FILE_PATH}\")\n",
    "            return pd.read_csv(CSV_FILE_PATH)\n",
    "        \n",
    "        # Fetch from database if cache doesn't exist\n",
    "        logger.info(\"Cache not found, fetching from database\")\n",
    "        df, error = fetch_data_from_db()\n",
    "        \n",
    "        if error:\n",
    "            raise RuntimeError(error)\n",
    "        \n",
    "        if df is not None and not df.empty:\n",
    "            # Save to cache\n",
    "            save_to_csv(df, CSV_FILE_PATH)\n",
    "            return df\n",
    "        else:\n",
    "            raise ValueError(\"No data retrieved from database\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in load_data: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "# Main execution\n",
    "try:\n",
    "    df = load_data()\n",
    "    logger.info(f\"DataFrame shape: {df.shape}\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to load data\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(296547, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wrkordr_wrk_rqstd</th>\n",
       "      <th>wrkordr_wrk_prfrmd</th>\n",
       "      <th>wrkordreqpmnt_wrk_rqstd</th>\n",
       "      <th>wrkordreqpmnt_wrk_prfrmd</th>\n",
       "      <th>wrkordrinvntry_dscrptn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PREVENTIVE MAINT  --  2012 JAN DEEP DIVE\\rFILL...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PREVENTIVE MAINT  --  2012 JAN DEEP DIVE\\rFILL...</td>\n",
       "      <td>ST713886-PERFORMED PER SCOPE-NO PROBLEMS NOTED</td>\n",
       "      <td>ICE MACHINE CLEANER NICKEL SAFE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PREVENTIVE MAINT  --  2012 JAN DEEP DIVE\\rFILL...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PREVENTIVE MAINT  --  2012 JAN DEEP DIVE\\rFILL...</td>\n",
       "      <td>ST713890 - PERFORMED PM PER SCOPE</td>\n",
       "      <td>ICE MACHINE CLEANER NICKEL SAFE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SERVICE CALL  --  2012 JAN DEEP DIVE\\rFILL OUT...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SERVICE CALL  --  2012 JAN DEEP DIVE\\rFILL OUT...</td>\n",
       "      <td>ST713888-PERFORMED PER SCOPE-NO PROBLEMS NOTED</td>\n",
       "      <td>ICE MACHINE CLEANER NICKEL SAFE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PREVENTIVE MAINT  --  2012 JAN DEEP DIVE\\rFILL...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PREVENTIVE MAINT  --  2012 JAN DEEP DIVE\\rFILL...</td>\n",
       "      <td>COMPLETED PER SCOPE - NO PROBLEMS NOTED - CMP ...</td>\n",
       "      <td>ICE MACHINE CLEANER NICKEL SAFE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ICE MACHINE NOT WORKING - MAKING LOUD NOISE WH...</td>\n",
       "      <td>I/M R/R COND FAN MOTOR</td>\n",
       "      <td>ICE MACHINE NOT WORKING - MAKING LOUD NOISE WH...</td>\n",
       "      <td>ST 212897 - REMOVED AND REPLACED COND FAN MOTO...</td>\n",
       "      <td>FAN MTR 240V 606/806/1006 3/4 MS X MS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   wrkordr_wrk_rqstd      wrkordr_wrk_prfrmd  \\\n",
       "0  PREVENTIVE MAINT  --  2012 JAN DEEP DIVE\\rFILL...                     NaN   \n",
       "1  PREVENTIVE MAINT  --  2012 JAN DEEP DIVE\\rFILL...                     NaN   \n",
       "2  SERVICE CALL  --  2012 JAN DEEP DIVE\\rFILL OUT...                     NaN   \n",
       "3  PREVENTIVE MAINT  --  2012 JAN DEEP DIVE\\rFILL...                     NaN   \n",
       "4  ICE MACHINE NOT WORKING - MAKING LOUD NOISE WH...  I/M R/R COND FAN MOTOR   \n",
       "\n",
       "                             wrkordreqpmnt_wrk_rqstd  \\\n",
       "0  PREVENTIVE MAINT  --  2012 JAN DEEP DIVE\\rFILL...   \n",
       "1  PREVENTIVE MAINT  --  2012 JAN DEEP DIVE\\rFILL...   \n",
       "2  SERVICE CALL  --  2012 JAN DEEP DIVE\\rFILL OUT...   \n",
       "3  PREVENTIVE MAINT  --  2012 JAN DEEP DIVE\\rFILL...   \n",
       "4  ICE MACHINE NOT WORKING - MAKING LOUD NOISE WH...   \n",
       "\n",
       "                            wrkordreqpmnt_wrk_prfrmd  \\\n",
       "0     ST713886-PERFORMED PER SCOPE-NO PROBLEMS NOTED   \n",
       "1                  ST713890 - PERFORMED PM PER SCOPE   \n",
       "2     ST713888-PERFORMED PER SCOPE-NO PROBLEMS NOTED   \n",
       "3  COMPLETED PER SCOPE - NO PROBLEMS NOTED - CMP ...   \n",
       "4  ST 212897 - REMOVED AND REPLACED COND FAN MOTO...   \n",
       "\n",
       "                  wrkordrinvntry_dscrptn  \n",
       "0        ICE MACHINE CLEANER NICKEL SAFE  \n",
       "1        ICE MACHINE CLEANER NICKEL SAFE  \n",
       "2        ICE MACHINE CLEANER NICKEL SAFE  \n",
       "3        ICE MACHINE CLEANER NICKEL SAFE  \n",
       "4  FAN MTR 240V 606/806/1006 3/4 MS X MS  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Configure logging if not already configured\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define columns to combine\n",
    "columns_to_combine = ['wrkordr_wrk_prfrmd', 'wrkordreqpmnt_wrk_prfrmd', 'wrkordrinvntry_dscrptn']\n",
    "\n",
    "# Pre-compile the regex pattern for better performance\n",
    "pattern = re.compile(r'[^a-zA-Z0-9\\s]|\\r')\n",
    "\n",
    "try:\n",
    "    # Make a copy of only source columns\n",
    "    df = df[columns_to_combine].copy()\n",
    "\n",
    "    # Perform operations in a memory-efficient way\n",
    "    df['combined_column'] = (\n",
    "        df[columns_to_combine]\n",
    "        .fillna('')  # Replace NaN with empty string\n",
    "        .astype(str)  # Convert to string type (more compatible than string[pyarrow])\n",
    "        .agg(' '.join, axis=1)  # Join columns with space\n",
    "        .str.replace(pattern, ' ', regex=True)  # Clean text using pre-compiled pattern\n",
    "    )\n",
    "\n",
    "    # Clean up extra whitespace\n",
    "    df['combined_column'] = df['combined_column'].str.strip().str.replace(r'\\s+', ' ')\n",
    "\n",
    "    # Optional: Free memory if the original columns are no longer needed\n",
    "    if columns_to_combine:\n",
    "        df.drop(columns=columns_to_combine, inplace=True)\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error processing DataFrame: {str(e)}\")\n",
    "    raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random sample of 100,000 records from the DataFrame\n",
    "df_sample = df.sample(n=100000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Python version: 3.11.10 (main, Sep  7 2024, 01:03:31) [Clang 16.0.0 (clang-1600.0.26.3)]\n",
      "INFO:__main__:PyTorch version: 2.5.1\n",
      "INFO:__main__:Transformers version: 4.46.2\n",
      "INFO:__main__:CUDA available: False\n",
      "INFO:__main__:MPS device detected\n",
      "INFO:__main__:Using device: mps\n",
      "INFO:__main__:Attempting to load model: vblagoje/bert-english-uncased-finetuned-pos\n",
      "INFO:__main__:Using cache directory: /Users/nikolay_tishchenko/.cache/huggingface\n",
      "INFO:__main__:Attempting to load tokenizer...\n",
      "loading configuration file config.json from cache at /Users/nikolay_tishchenko/.cache/huggingface/models--vblagoje--bert-english-uncased-finetuned-pos/snapshots/46ec120264b121e8d92bef19b45c107d06d2cb99/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"vblagoje/bert-english-uncased-finetuned-pos\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"ADJ\",\n",
      "    \"1\": \"ADP\",\n",
      "    \"2\": \"ADV\",\n",
      "    \"3\": \"AUX\",\n",
      "    \"4\": \"CCONJ\",\n",
      "    \"5\": \"DET\",\n",
      "    \"6\": \"INTJ\",\n",
      "    \"7\": \"NOUN\",\n",
      "    \"8\": \"NUM\",\n",
      "    \"9\": \"PART\",\n",
      "    \"10\": \"PRON\",\n",
      "    \"11\": \"PROPN\",\n",
      "    \"12\": \"PUNCT\",\n",
      "    \"13\": \"SCONJ\",\n",
      "    \"14\": \"SYM\",\n",
      "    \"15\": \"VERB\",\n",
      "    \"16\": \"X\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"ADJ\": 0,\n",
      "    \"ADP\": 1,\n",
      "    \"ADV\": 2,\n",
      "    \"AUX\": 3,\n",
      "    \"CCONJ\": 4,\n",
      "    \"DET\": 5,\n",
      "    \"INTJ\": 6,\n",
      "    \"NOUN\": 7,\n",
      "    \"NUM\": 8,\n",
      "    \"PART\": 9,\n",
      "    \"PRON\": 10,\n",
      "    \"PROPN\": 11,\n",
      "    \"PUNCT\": 12,\n",
      "    \"SCONJ\": 13,\n",
      "    \"SYM\": 14,\n",
      "    \"VERB\": 15,\n",
      "    \"X\": 16\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /Users/nikolay_tishchenko/.cache/huggingface/models--vblagoje--bert-english-uncased-finetuned-pos/snapshots/46ec120264b121e8d92bef19b45c107d06d2cb99/vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /Users/nikolay_tishchenko/.cache/huggingface/models--vblagoje--bert-english-uncased-finetuned-pos/snapshots/46ec120264b121e8d92bef19b45c107d06d2cb99/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /Users/nikolay_tishchenko/.cache/huggingface/models--vblagoje--bert-english-uncased-finetuned-pos/snapshots/46ec120264b121e8d92bef19b45c107d06d2cb99/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/nikolay_tishchenko/.cache/huggingface/models--vblagoje--bert-english-uncased-finetuned-pos/snapshots/46ec120264b121e8d92bef19b45c107d06d2cb99/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"vblagoje/bert-english-uncased-finetuned-pos\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"ADJ\",\n",
      "    \"1\": \"ADP\",\n",
      "    \"2\": \"ADV\",\n",
      "    \"3\": \"AUX\",\n",
      "    \"4\": \"CCONJ\",\n",
      "    \"5\": \"DET\",\n",
      "    \"6\": \"INTJ\",\n",
      "    \"7\": \"NOUN\",\n",
      "    \"8\": \"NUM\",\n",
      "    \"9\": \"PART\",\n",
      "    \"10\": \"PRON\",\n",
      "    \"11\": \"PROPN\",\n",
      "    \"12\": \"PUNCT\",\n",
      "    \"13\": \"SCONJ\",\n",
      "    \"14\": \"SYM\",\n",
      "    \"15\": \"VERB\",\n",
      "    \"16\": \"X\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"ADJ\": 0,\n",
      "    \"ADP\": 1,\n",
      "    \"ADV\": 2,\n",
      "    \"AUX\": 3,\n",
      "    \"CCONJ\": 4,\n",
      "    \"DET\": 5,\n",
      "    \"INTJ\": 6,\n",
      "    \"NOUN\": 7,\n",
      "    \"NUM\": 8,\n",
      "    \"PART\": 9,\n",
      "    \"PRON\": 10,\n",
      "    \"PROPN\": 11,\n",
      "    \"PUNCT\": 12,\n",
      "    \"SCONJ\": 13,\n",
      "    \"SYM\": 14,\n",
      "    \"VERB\": 15,\n",
      "    \"X\": 16\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /Users/nikolay_tishchenko/.cache/huggingface/models--vblagoje--bert-english-uncased-finetuned-pos/snapshots/46ec120264b121e8d92bef19b45c107d06d2cb99/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"vblagoje/bert-english-uncased-finetuned-pos\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"ADJ\",\n",
      "    \"1\": \"ADP\",\n",
      "    \"2\": \"ADV\",\n",
      "    \"3\": \"AUX\",\n",
      "    \"4\": \"CCONJ\",\n",
      "    \"5\": \"DET\",\n",
      "    \"6\": \"INTJ\",\n",
      "    \"7\": \"NOUN\",\n",
      "    \"8\": \"NUM\",\n",
      "    \"9\": \"PART\",\n",
      "    \"10\": \"PRON\",\n",
      "    \"11\": \"PROPN\",\n",
      "    \"12\": \"PUNCT\",\n",
      "    \"13\": \"SCONJ\",\n",
      "    \"14\": \"SYM\",\n",
      "    \"15\": \"VERB\",\n",
      "    \"16\": \"X\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"ADJ\": 0,\n",
      "    \"ADP\": 1,\n",
      "    \"ADV\": 2,\n",
      "    \"AUX\": 3,\n",
      "    \"CCONJ\": 4,\n",
      "    \"DET\": 5,\n",
      "    \"INTJ\": 6,\n",
      "    \"NOUN\": 7,\n",
      "    \"NUM\": 8,\n",
      "    \"PART\": 9,\n",
      "    \"PRON\": 10,\n",
      "    \"PROPN\": 11,\n",
      "    \"PUNCT\": 12,\n",
      "    \"SCONJ\": 13,\n",
      "    \"SYM\": 14,\n",
      "    \"VERB\": 15,\n",
      "    \"X\": 16\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:__main__:Tokenizer loaded successfully\n",
      "INFO:__main__:Attempting to load model...\n",
      "loading configuration file config.json from cache at /Users/nikolay_tishchenko/.cache/huggingface/models--vblagoje--bert-english-uncased-finetuned-pos/snapshots/46ec120264b121e8d92bef19b45c107d06d2cb99/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"vblagoje/bert-english-uncased-finetuned-pos\",\n",
      "  \"architectures\": [\n",
      "    \"BertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"ADJ\",\n",
      "    \"1\": \"ADP\",\n",
      "    \"2\": \"ADV\",\n",
      "    \"3\": \"AUX\",\n",
      "    \"4\": \"CCONJ\",\n",
      "    \"5\": \"DET\",\n",
      "    \"6\": \"INTJ\",\n",
      "    \"7\": \"NOUN\",\n",
      "    \"8\": \"NUM\",\n",
      "    \"9\": \"PART\",\n",
      "    \"10\": \"PRON\",\n",
      "    \"11\": \"PROPN\",\n",
      "    \"12\": \"PUNCT\",\n",
      "    \"13\": \"SCONJ\",\n",
      "    \"14\": \"SYM\",\n",
      "    \"15\": \"VERB\",\n",
      "    \"16\": \"X\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"ADJ\": 0,\n",
      "    \"ADP\": 1,\n",
      "    \"ADV\": 2,\n",
      "    \"AUX\": 3,\n",
      "    \"CCONJ\": 4,\n",
      "    \"DET\": 5,\n",
      "    \"INTJ\": 6,\n",
      "    \"NOUN\": 7,\n",
      "    \"NUM\": 8,\n",
      "    \"PART\": 9,\n",
      "    \"PRON\": 10,\n",
      "    \"PROPN\": 11,\n",
      "    \"PUNCT\": 12,\n",
      "    \"SCONJ\": 13,\n",
      "    \"SYM\": 14,\n",
      "    \"VERB\": 15,\n",
      "    \"X\": 16\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.46.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/nikolay_tishchenko/.cache/huggingface/models--vblagoje--bert-english-uncased-finetuned-pos/snapshots/46ec120264b121e8d92bef19b45c107d06d2cb99/pytorch_model.bin\n",
      "Attempting to create safetensors variant\n",
      "Some weights of the model checkpoint at vblagoje/bert-english-uncased-finetuned-pos were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertForTokenClassification were initialized from the model checkpoint at vblagoje/bert-english-uncased-finetuned-pos.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\n",
      "INFO:__main__:Model loaded successfully\n",
      "INFO:__main__:Moving model to device: mps\n",
      "INFO:__main__:Successfully loaded model: vblagoje/bert-english-uncased-finetuned-pos\n",
      "INFO:__main__:Model initialized successfully\n"
     ]
    }
   ],
   "source": [
    "from functools import lru_cache\n",
    "import torch\n",
    "from transformers import (\n",
    "    BertTokenizer, \n",
    "    BertForTokenClassification, \n",
    "    logging as transformers_logging,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification\n",
    ")\n",
    "import logging\n",
    "from typing import Tuple, Optional\n",
    "import os\n",
    "import sys\n",
    "from transformers import __version__ as transformers_version\n",
    "\n",
    "# Configure detailed logging\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,  # Changed to DEBUG for more detailed logs\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "transformers_logging.set_verbosity_info()\n",
    "\n",
    "def check_environment():\n",
    "    \"\"\"Check and log environment details.\"\"\"\n",
    "    logger.info(f\"Python version: {sys.version}\")\n",
    "    logger.info(f\"PyTorch version: {torch.__version__}\")\n",
    "    logger.info(f\"Transformers version: {transformers_version}\")\n",
    "    logger.info(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        logger.info(f\"CUDA version: {torch.version.cuda}\")\n",
    "        logger.info(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"Determine the optimal device with caching.\"\"\"\n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            logger.info(\"CUDA device detected\")\n",
    "            return torch.device(\"cuda\")\n",
    "        elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "            logger.info(\"MPS device detected\")\n",
    "            return torch.device(\"mps\")\n",
    "        logger.info(\"Using CPU device\")\n",
    "        return torch.device(\"cpu\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error detecting device, defaulting to CPU: {str(e)}\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "def load_model_components(model_name: str, device: torch.device) -> Tuple[Optional[BertTokenizer], Optional[BertForTokenClassification]]:\n",
    "    \"\"\"Load model components with optimizations and proper error handling.\"\"\"\n",
    "    try:\n",
    "        # Set up caching directory\n",
    "        cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"huggingface\")\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        logger.info(f\"Using cache directory: {cache_dir}\")\n",
    "\n",
    "        # Try loading with Auto classes first\n",
    "        logger.info(\"Attempting to load tokenizer...\")\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_name,\n",
    "                cache_dir=cache_dir,\n",
    "                use_fast=True,\n",
    "                model_max_length=512\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"AutoTokenizer failed, trying BertTokenizer: {str(e)}\")\n",
    "            tokenizer = BertTokenizer.from_pretrained(\n",
    "                model_name,\n",
    "                cache_dir=cache_dir,\n",
    "                use_fast=True,\n",
    "                model_max_length=512\n",
    "            )\n",
    "        logger.info(\"Tokenizer loaded successfully\")\n",
    "\n",
    "        # Load model with Auto class\n",
    "        logger.info(\"Attempting to load model...\")\n",
    "        try:\n",
    "            model = AutoModelForTokenClassification.from_pretrained(\n",
    "                model_name,\n",
    "                cache_dir=cache_dir,\n",
    "                return_dict=True,  # Changed to True for better compatibility\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"AutoModel failed, trying BertForTokenClassification: {str(e)}\")\n",
    "            model = BertForTokenClassification.from_pretrained(\n",
    "                model_name,\n",
    "                cache_dir=cache_dir,\n",
    "                return_dict=True,\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "        logger.info(\"Model loaded successfully\")\n",
    "\n",
    "        # Move model to device and optimize\n",
    "        logger.info(f\"Moving model to device: {device}\")\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        # Disable gradient computation for inference\n",
    "        torch.set_grad_enabled(False)\n",
    "        \n",
    "        return tokenizer, model\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model components: {str(e)}\", exc_info=True)\n",
    "        return None, None\n",
    "\n",
    "def initialize_model() -> Tuple[Optional[BertTokenizer], Optional[BertForTokenClassification], Optional[torch.device]]:\n",
    "    \"\"\"Initialize model with proper error handling and optimizations.\"\"\"\n",
    "    try:\n",
    "        # Check environment first\n",
    "        check_environment()\n",
    "        \n",
    "        # Get device (cached)\n",
    "        device = get_device()\n",
    "        logger.info(f\"Using device: {device}\")\n",
    "\n",
    "        # Try alternative model if the original fails\n",
    "        model_names = [\n",
    "            'vblagoje/bert-english-uncased-finetuned-pos',\n",
    "            'bert-base-uncased'  # Fallback model\n",
    "        ]\n",
    "        \n",
    "        for model_name in model_names:\n",
    "            logger.info(f\"Attempting to load model: {model_name}\")\n",
    "            tokenizer, model = load_model_components(model_name, device)\n",
    "            \n",
    "            if tokenizer is not None and model is not None:\n",
    "                logger.info(f\"Successfully loaded model: {model_name}\")\n",
    "                return tokenizer, model, device\n",
    "            \n",
    "            logger.warning(f\"Failed to load model: {model_name}, trying next option...\")\n",
    "        \n",
    "        raise RuntimeError(\"All model loading attempts failed\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error initializing model: {str(e)}\", exc_info=True)\n",
    "        return None, None, None\n",
    "\n",
    "# Initialize the model with proper error handling\n",
    "try:\n",
    "    # Clear any existing cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    tokenizer, model, device = initialize_model()\n",
    "    if None in (tokenizer, model, device):\n",
    "        raise RuntimeError(\"Model initialization failed\")\n",
    "    logger.info(\"Model initialized successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to initialize model\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import re\n",
    "from typing import List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ProcessingContext:\n",
    "    current_verb: Optional[str] = None\n",
    "    current_objects: List[str] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.current_objects is None:\n",
    "            self.current_objects = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_verb = None\n",
    "        self.current_objects.clear()\n",
    "\n",
    "@lru_cache(maxsize=1024)\n",
    "def is_potential_verb(word: str, tag: str) -> bool:\n",
    "    \"\"\"Cached check for potential verbs.\"\"\"\n",
    "    return (tag.startswith('VB') or tag == 'VBD' or tag == 'VBN' or \n",
    "            (tag == 'JJ' and (word.endswith('ed') or word.endswith('en'))))\n",
    "\n",
    "@lru_cache(maxsize=1024)\n",
    "def is_object_component(tag: str, word: str) -> bool:\n",
    "    \"\"\"Cached check for object components.\"\"\"\n",
    "    return (tag.startswith('NN') or \n",
    "            (tag.startswith('JJ') and not word.endswith('ed')) or \n",
    "            tag == 'IN' or tag.startswith('VBG'))\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"Preprocess text with compiled regex patterns.\"\"\"\n",
    "    # Compile regex patterns once\n",
    "    PATTERNS = {\n",
    "        'as_needed': re.compile(r'\\bas needed\\b'),\n",
    "        'conjunctions': re.compile(r'\\s*,\\s*and\\s+'),\n",
    "        'whitespace': re.compile(r'\\s+')\n",
    "    }\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = PATTERNS['as_needed'].sub('', text)\n",
    "    text = PATTERNS['conjunctions'].sub(', ', text)\n",
    "    return PATTERNS['whitespace'].sub(' ', text).strip()\n",
    "\n",
    "def process_compound_objects(tagged: List[Tuple[str, str]], start_idx: int) -> Tuple[List[str], int]:\n",
    "    \"\"\"Process compound objects and return objects and new index.\"\"\"\n",
    "    objects = [tagged[start_idx][0]]\n",
    "    i = start_idx + 1\n",
    "    \n",
    "    while i < len(tagged) and is_object_component(tagged[i][1], tagged[i][0]):\n",
    "        objects.append(tagged[i][0])\n",
    "        i += 1\n",
    "    \n",
    "    return objects, i - 1\n",
    "\n",
    "def process_text(text: str, tokenizer, model, device) -> List[Tuple[str, str]]:\n",
    "    try:\n",
    "        # Initialize tools lazily (only when needed)\n",
    "        spell = SpellChecker()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        # Preprocess text\n",
    "        text = preprocess_text(text)\n",
    "        \n",
    "        # Process text\n",
    "        tokens = word_tokenize(text)\n",
    "        tagged = pos_tag(tokens)\n",
    "        \n",
    "        verb_object_pairs = []\n",
    "        context = ProcessingContext()\n",
    "        \n",
    "        def add_pair():\n",
    "            if context.current_verb and context.current_objects:\n",
    "                lemmatized_verb = lemmatizer.lemmatize(context.current_verb, 'v')\n",
    "                verb_object_pairs.append((lemmatized_verb, ' '.join(context.current_objects)))\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(tagged):\n",
    "            word, tag = tagged[i]\n",
    "            \n",
    "            if is_potential_verb(word, tag):\n",
    "                # Save previous pair if exists\n",
    "                add_pair()\n",
    "                context.current_verb = spell.correction(word)\n",
    "                context.current_objects.clear()\n",
    "                \n",
    "            elif is_object_component(tag, word):\n",
    "                if context.current_verb:\n",
    "                    temp_objects, new_idx = process_compound_objects(tagged, i)\n",
    "                    if any(tagged[j][1].startswith('NN') for j in range(i, new_idx + 1)):\n",
    "                        context.current_objects.extend(temp_objects)\n",
    "                    i = new_idx\n",
    "                    \n",
    "            elif word == ',':\n",
    "                add_pair()\n",
    "                context.reset()\n",
    "            \n",
    "            i += 1\n",
    "        \n",
    "        # Add final pair if exists\n",
    "        add_pair()\n",
    "        \n",
    "        return verb_object_pairs\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Error processing text: {str(e)}\")\n",
    "        print(f\"Traceback: {traceback.format_exc()}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_task(verb,obj):\n",
    "    \"\"\"Categorize a task based on its verb into one of four categories: Inspection, Cleaning, Maintenance, or Other.\"\"\"\n",
    "    inspection_verbs = {'inspect', 'check', 'examine', 'monitor', 'observe', 'verify', 'test', 'diagnose', 'assess'}\n",
    "    cleaning_verbs = {'clean', 'brush', 'wash', 'wipe', 'descale', 'sanitize', 'flush', 'clear'}\n",
    "    maintenance_verbs = {'lubricate', 'adjust', 'replace', 'repair', 'calibrate', 'install', 'fix', 'tighten', 'service'}\n",
    "    \n",
    "    verb = verb.lower()\n",
    "    if verb in inspection_verbs:\n",
    "        return \"Inspection\"\n",
    "    elif verb in cleaning_verbs:\n",
    "        return \"Cleaning\"\n",
    "    elif verb in maintenance_verbs:\n",
    "        return \"Maintenance\"\n",
    "    else:\n",
    "        return \"Other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to convert .bin model on the fly to safetensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to process maintenance records...\n",
      "Total records to process: 100000\n"
     ]
    }
   ],
   "source": [
    "# Process all records in df_sample['combined_column']\n",
    "from collections import Counter\n",
    "\n",
    "print(\"Starting to process maintenance records...\")\n",
    "\n",
    "# Initialize counters for tasks and categories\n",
    "all_verb_object_pairs = []  # Keep as list to track frequencies\n",
    "task_frequencies = Counter()\n",
    "\n",
    "# Process each record\n",
    "total_records = len(df_sample['combined_column'].dropna())\n",
    "print(f\"Total records to process: {total_records}\")\n",
    "\n",
    "for idx, text in enumerate(df_sample['combined_column'].dropna(), 1):\n",
    "    if idx % (total_records // 20) == 0:  # Print progress every 5%\n",
    "        print(f\"Processing record {idx}/{total_records} ({(idx/total_records*100):.1f}%)\")\n",
    "    \n",
    "    pairs = process_text(text, tokenizer, model, device)\n",
    "    if pairs:  # Only add if we got valid pairs\n",
    "        for pair in pairs:\n",
    "            task_frequencies[tuple(pair)] += 1\n",
    "            all_verb_object_pairs.append(tuple(pair))\n",
    "\n",
    "print(f\"\\nTotal task occurrences: {len(all_verb_object_pairs)}\")\n",
    "print(f\"Unique tasks: {len(task_frequencies)}\")\n",
    "\n",
    "# Group tasks by category with frequencies\n",
    "tasks_by_category = {}\n",
    "category_totals = {}\n",
    "\n",
    "for (verb, obj), freq in task_frequencies.most_common():\n",
    "    category = categorize_task(verb, obj)\n",
    "    if category not in tasks_by_category:\n",
    "        tasks_by_category[category] = []\n",
    "        category_totals[category] = 0\n",
    "    tasks_by_category[category].append((verb, obj, freq))\n",
    "    category_totals[category] += freq\n",
    "\n",
    "# Print tasks by category with frequencies\n",
    "print(\"\\nTask Categories Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "total_tasks = sum(category_totals.values())\n",
    "\n",
    "for category in [\"Other\", \"Maintenance\", \"Inspection\", \"Cleaning\"]:\n",
    "    if category in tasks_by_category:\n",
    "        print(f\"\\n{category} Tasks ({category_totals[category]} occurrences, {(category_totals[category]/total_tasks*100):.1f}%)\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Sort tasks by frequency within category\n",
    "        sorted_tasks = sorted(tasks_by_category[category], key=lambda x: (-x[2], x[0]))\n",
    "        \n",
    "        for verb, obj, freq in sorted_tasks:\n",
    "            percentage = (freq / category_totals[category]) * 100\n",
    "            print(f\"â€¢ {verb.capitalize()}: {obj} ({freq} times, {percentage:.1f}% of {category})\")\n",
    "\n",
    "# Print overall statistics\n",
    "print(\"\\nTask Distribution Statistics\")\n",
    "print(\"=\" * 50)\n",
    "for category, total in category_totals.items():\n",
    "    percentage = (total / total_tasks) * 100\n",
    "    unique_tasks = len(tasks_by_category[category])\n",
    "    print(f\"{category}: {total} occurrences ({percentage:.1f}%), {unique_tasks} unique tasks\")\n",
    "\n",
    "# Create a summary DataFrame\n",
    "summary_data = {\n",
    "    'Category': [],\n",
    "    'Total_Occurrences': [],\n",
    "    'Unique_Tasks': [],\n",
    "    'Percentage': []\n",
    "}\n",
    "\n",
    "for category in tasks_by_category:\n",
    "    summary_data['Category'].append(category)\n",
    "    summary_data['Total_Occurrences'].append(category_totals[category])\n",
    "    summary_data['Unique_Tasks'].append(len(tasks_by_category[category]))\n",
    "    summary_data['Percentage'].append((category_totals[category] / total_tasks) * 100)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nSummary DataFrame:\")\n",
    "print(\"=\" * 50)\n",
    "print(summary_df.to_string(index=False, float_format=lambda x: '{:.1f}'.format(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Set pandas display options to prevent wrapping and ensure consistent formatting\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:,}')\n",
    "\n",
    "# Create DataFrames for each category\n",
    "category_dfs = {}\n",
    "for category in ['Maintenance', 'Inspection', 'Cleaning', 'Other']:\n",
    "    category_tasks = []\n",
    "    for (verb, obj), freq in task_frequencies.items():\n",
    "        task_category = categorize_task(verb,obj)\n",
    "        if task_category == category:\n",
    "            category_tasks.append({\n",
    "                'Verb': verb,\n",
    "                'Object': obj,\n",
    "                'Frequency': freq,\n",
    "                'Task Description': f\"{verb} {obj}\".strip()  # Ensure no extra spaces\n",
    "            })\n",
    "    \n",
    "    if category_tasks:  # Only create DataFrame if there are tasks\n",
    "        category_dfs[category] = pd.DataFrame(category_tasks)\n",
    "        category_dfs[category] = category_dfs[category].sort_values('Frequency', ascending=False)\n",
    "        \n",
    "        print(f\"\\n{category} Tasks:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Format the DataFrame display\n",
    "        df_display = category_dfs[category].head(10).copy()\n",
    "        df_display.index = range(1, len(df_display) + 1)  # Start index from 1\n",
    "        print(df_display.to_string(\n",
    "            justify='left',\n",
    "            col_space={\n",
    "                'Verb': 20,\n",
    "                'Object': 40,\n",
    "                'Frequency': 15,\n",
    "                'Task Description': 45\n",
    "            }\n",
    "        ))\n",
    "\n",
    "# Save DataFrames to CSV files (optional)\n",
    "for category, df in category_dfs.items():\n",
    "    if not df.empty:\n",
    "        filename = f\"{category.lower()}_tasks.csv\"\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"\\nSaved {category} tasks to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary of all tasks\n",
    "print(f\"\\nMaintenance Tasks Analysis Summary ({pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')})\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# Create a combined DataFrame of all tasks\n",
    "all_tasks = []\n",
    "for category, df in category_dfs.items():\n",
    "    df = df.copy()\n",
    "    df['Category'] = category\n",
    "    all_tasks.append(df)\n",
    "all_tasks_df = pd.concat(all_tasks, ignore_index=True)\n",
    "\n",
    "# Create summary DataFrame with formatting\n",
    "summary_df = all_tasks_df.groupby('Category').agg({\n",
    "    'Task Description': 'count',\n",
    "    'Frequency': 'sum'\n",
    "}).rename(columns={\n",
    "    'Task Description': 'Unique Tasks',\n",
    "    'Frequency': 'Total Occurrences'\n",
    "})\n",
    "\n",
    "# Add percentage columns\n",
    "total_tasks = summary_df['Unique Tasks'].sum()\n",
    "total_occurrences = summary_df['Total Occurrences'].sum()\n",
    "\n",
    "summary_df['% of Tasks'] = (summary_df['Unique Tasks'] / total_tasks * 100).round(1)\n",
    "summary_df['% of Occurrences'] = (summary_df['Total Occurrences'] / total_occurrences * 100).round(1)\n",
    "\n",
    "# Format numbers with thousands separator\n",
    "summary_df['Unique Tasks'] = summary_df['Unique Tasks'].apply(lambda x: f\"{x:,}\")\n",
    "summary_df['Total Occurrences'] = summary_df['Total Occurrences'].apply(lambda x: f\"{x:,}\")\n",
    "\n",
    "\n",
    "# Sort categories in logical order\n",
    "category_order = ['Maintenance', 'Inspection', 'Cleaning', 'Other']\n",
    "summary_df = summary_df.reindex(category_order)\n",
    "\n",
    "# Set display options for better formatting\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:,.0f}' if x.is_integer() else f'{x:.1f}%')\n",
    "\n",
    "# Print formatted DataFrame\n",
    "print(summary_df)\n",
    "print(\"=\" * 120)\n",
    "print(f\"Total Unique Tasks: {total_tasks:,.0f}\")\n",
    "print(f\"Total Task Occurrences: {total_occurrences:,.0f}\")\n",
    "print(\"=\" * 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tasks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coolsys_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
