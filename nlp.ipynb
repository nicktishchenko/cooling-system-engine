{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install packages\n",
    "\n",
    "# %pip install pandas scikit-learn\n",
    "# %pip install transformers torch\n",
    "# %pip install --upgrade jupyter ipywidgets\n",
    "# %pip install nltk\n",
    "# %pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nikolay_tishchenko/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nikolay_tishchenko/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nikolay_tishchenko/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Import Libraries\n",
    "import os\n",
    "import platform\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pyodbc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline, BertTokenizer, BertModel, AutoTokenizer\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import textwrap\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Check if 'wordnet' and 'averaged_perceptron_tagger' are already downloaded, and download them if necessary\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify action verbs and related nouns to build 'root causes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NLTK and download necessary resources\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/nikolay_tishchenko/nltk_data', '/Users/nikolay_tishchenko/bitbucket_projects/coolsys/.venv/nltk_data', '/Users/nikolay_tishchenko/bitbucket_projects/coolsys/.venv/share/nltk_data', '/Users/nikolay_tishchenko/bitbucket_projects/coolsys/.venv/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nikolay_tishchenko/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nikolay_tishchenko/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nikolay_tishchenko/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk_data_path = '/Users/nikolay_tishchenko/nltk_data'\n",
    "nltk.download('punkt', download_dir=nltk_data_path)\n",
    "nltk.download('wordnet', download_dir=nltk_data_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir=nltk_data_path)\n",
    "\n",
    "# Verify the resources have been downloaded\n",
    "print(nltk.data.path)   # Print the NLTK data path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/nikolay_tishchenko/nltk_data', '/Users/nikolay_tishchenko/bitbucket_projects/coolsys/.venv/nltk_data', '/Users/nikolay_tishchenko/bitbucket_projects/coolsys/.venv/share/nltk_data', '/Users/nikolay_tishchenko/bitbucket_projects/coolsys/.venv/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Define the custom path for nltk_data\n",
    "nltk_data_path = '/Users/nikolay_tishchenko/nltk_data'\n",
    "\n",
    "# Add the custom path to nltk.data.path\n",
    "if nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "# Verify the custom path is added\n",
    "print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified action verbs:\n",
      "['brushed', 'arrived', 'operating', 'Checked', 'needed', 'discovered', 'Feed', 'Replaced', 'working', 'exhaust', 'was', 'be', 'checked', 'am', 'performed', 'cleane', 'inspecfted', 'monitored', 'inspected', 'walk', 'is', 'found', 'tested', 'sensing', 'report', 'replaced', 'resolve', 'troubleshooting', 'adjusted', 'lubricated', 'expected', 'cleaned']\n",
      "Verified action verbs:\n",
      "['brushed', 'arrived', 'operating', 'Checked', 'needed', 'discovered', 'Feed', 'Replaced', 'working', 'exhaust', 'was', 'be', 'checked', 'am', 'performed', 'monitored', 'inspected', 'walk', 'is', 'found', 'tested', 'sensing', 'report', 'replaced', 'resolve', 'troubleshooting', 'adjusted', 'lubricated', 'expected', 'cleaned']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Define the text to analyze\n",
    "text = \"\"\"\n",
    "performed pm per scope hvac tasks replaced air filters visually inspected for refrigerant leaks lubricated all bearings as needed shut off water supply  \n",
    "fall visit as needed  inspected belts and adjusted as needed cleaned condensate lines checked contactors visually inspected evaporator and condenser coils \n",
    "inspected and tested heating circuit for operation inspected and tested each condensate pump if applicable refrigerations tasks inspected all hinges and \n",
    "gaskets checked electrical contacts  controls and components inspected relays and contactors visually inspected for leaks cleaned all pre filter media and \n",
    "replaced if applicable brushed coil fins as needed inspected blower wheels and fans lubricated motors and bearings as needed checked controls calibration and \n",
    "operation walk in tasks inspecfted all hinges and gaskets checked electrical contacts controls and components inspected relays and contactors visually inspected \n",
    "for leaks cleane dall pre filter media and replaced if applicable brushed coil fins as needed inspected blower wheels and fans lubricated motors and bearings \n",
    "inspected sight glass checked control calibration and operation ice machine tasks checked ice production thickness and sensing probes inspected door and gasket \n",
    "visually inspected bin and bin tstat inspected water pumps and distribution tubes visually inspected water filters inspected for any leaks cleaned condenser coils \n",
    "inspected all electrical components inspected fan motor  blades   bearings   lubricated as needed descaled ice machine exhaust fan tasks inspected roof fan belts and \n",
    "sheaves adjusted as needed replaced belts as needed  20x25x2 pleated filter high efficiency                                                                                                                                                       2023 Today  I arrived on site at unit IM01A and found that it was not operating  After troubleshooting  I discovered the following issues     Unit was off with a low water alarm    Water fill valve needs to be replaced   To resolve these issues  the following work was performed     Replaced the water fill valve    Replaced the water regulator    Checked over all operations and monitored the system   I am happy to report that the unit is now working normally  Upon departure  the unit was operating as expected  Solenoid  Water Feed 115V\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Perform POS tagging\n",
    "tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "# Extract verbs from the POS tagged tokens\n",
    "action_verbs = {word for word, pos in tagged_tokens if pos.startswith('VB')}\n",
    "\n",
    "# Verify that all values in action_verbs are actually verbs using nltk\n",
    "def is_verb(word):\n",
    "    synsets = wordnet.synsets(word)\n",
    "    if not synsets:\n",
    "        return False\n",
    "    return any(ss.pos() == 'v' for ss in synsets)\n",
    "\n",
    "# Filter out non-verbs\n",
    "verified_action_verbs = [verb for verb in action_verbs if is_verb(verb)]\n",
    "\n",
    "# Print the identified action verbs\n",
    "print(\"Identified action verbs:\")\n",
    "print(list(action_verbs))\n",
    "\n",
    "# Print the verified action verbs\n",
    "print(\"Verified action verbs:\")\n",
    "print(verified_action_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified action verbs:\n",
      "['arrive', 'discover', 'troubleshoot', 'sense', 'adjust', 'operate', 'Checked', 'Feed', 'work', 'Replaced', 'lubricate', 'exhaust', 'be', 'need', 'cleane', 'inspecfted', 'monitor', 'perform', 'walk', 'check', 'brush', 'replace', 'clean', 'report', 'resolve', 'test', 'expect', 'inspect', 'find']\n",
      "Verified action verbs:\n",
      "['arrive', 'discover', 'troubleshoot', 'sense', 'adjust', 'operate', 'Checked', 'Feed', 'work', 'Replaced', 'lubricate', 'exhaust', 'be', 'need', 'monitor', 'perform', 'walk', 'check', 'brush', 'replace', 'clean', 'report', 'resolve', 'test', 'expect', 'inspect', 'find']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Define the text to analyze\n",
    "text = \"\"\"\n",
    "performed pm per scope hvac tasks replaced air filters visually inspected for refrigerant leaks lubricated all bearings as needed shut off water supply  \n",
    "fall visit as needed  inspected belts and adjusted as needed cleaned condensate lines checked contactors visually inspected evaporator and condenser coils \n",
    "inspected and tested heating circuit for operation inspected and tested each condensate pump if applicable refrigerations tasks inspected all hinges and \n",
    "gaskets checked electrical contacts  controls and components inspected relays and contactors visually inspected for leaks cleaned all pre filter media and \n",
    "replaced if applicable brushed coil fins as needed inspected blower wheels and fans lubricated motors and bearings as needed checked controls calibration and \n",
    "operation walk in tasks inspecfted all hinges and gaskets checked electrical contacts controls and components inspected relays and contactors visually inspected \n",
    "for leaks cleane dall pre filter media and replaced if applicable brushed coil fins as needed inspected blower wheels and fans lubricated motors and bearings \n",
    "inspected sight glass checked control calibration and operation ice machine tasks checked ice production thickness and sensing probes inspected door and gasket \n",
    "visually inspected bin and bin tstat inspected water pumps and distribution tubes visually inspected water filters inspected for any leaks cleaned condenser coils \n",
    "inspected all electrical components inspected fan motor  blades   bearings   lubricated as needed descaled ice machine exhaust fan tasks inspected roof fan belts and \n",
    "sheaves adjusted as needed replaced belts as needed  20x25x2 pleated filter high efficiency                                                                                                                                                       2023 Today  I arrived on site at unit IM01A and found that it was not operating  After troubleshooting  I discovered the following issues     Unit was off with a low water alarm    Water fill valve needs to be replaced   To resolve these issues  the following work was performed     Replaced the water fill valve    Replaced the water regulator    Checked over all operations and monitored the system   I am happy to report that the unit is now working normally  Upon departure  the unit was operating as expected  Solenoid  Water Feed 115V\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Perform POS tagging\n",
    "tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "# Initialize the WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to convert POS tag to WordNet POS tag\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Extract verbs from the POS tagged tokens and lemmatize them\n",
    "action_verbs = set()\n",
    "for word, pos in tagged_tokens:\n",
    "    wordnet_pos = get_wordnet_pos(pos) or wordnet.NOUN\n",
    "    lemma = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
    "    if wordnet_pos == wordnet.VERB:\n",
    "        action_verbs.add(lemma)\n",
    "\n",
    "# Verify that all values in action_verbs are actually verbs using nltk\n",
    "def is_verb(word):\n",
    "    synsets = wordnet.synsets(word, pos=wordnet.VERB)\n",
    "    return bool(synsets)\n",
    "\n",
    "# Filter out non-verbs\n",
    "verified_action_verbs = [verb for verb in action_verbs if is_verb(verb)]\n",
    "\n",
    "# Print the identified action verbs\n",
    "print(\"Identified action verbs:\")\n",
    "print(list(action_verbs))\n",
    "\n",
    "# Print the verified action verbs\n",
    "print(\"Verified action verbs:\")\n",
    "print(verified_action_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified action verbs:\n",
      "['arrive', 'discover', 'troubleshoot', 'sense', 'adjust', 'operate', 'Checked', 'Feed', 'work', 'Replaced', 'lubricate', 'exhaust', 'be', 'need', 'cleane', 'inspecfted', 'monitor', 'perform', 'walk', 'check', 'brush', 'replace', 'clean', 'report', 'resolve', 'test', 'expect', 'inspect', 'find']\n",
      "Verified action verbs:\n",
      "['arrive', 'discover', 'troubleshoot', 'sense', 'adjust', 'operate', 'Checked', 'Feed', 'work', 'Replaced', 'lubricate', 'exhaust', 'be', 'need', 'monitor', 'perform', 'walk', 'check', 'brush', 'replace', 'clean', 'report', 'resolve', 'test', 'expect', 'inspect', 'find']\n",
      "Verified noun-verb pairs:\n",
      "[('work', 'be')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Define the text to analyze\n",
    "text = \"\"\"\n",
    "performed pm per scope hvac tasks replaced air filters visually inspected for refrigerant leaks lubricated all bearings as needed shut off water supply  \n",
    "fall visit as needed  inspected belts and adjusted as needed cleaned condensate lines checked contactors visually inspected evaporator and condenser coils \n",
    "inspected and tested heating circuit for operation inspected and tested each condensate pump if applicable refrigerations tasks inspected all hinges and \n",
    "gaskets checked electrical contacts  controls and components inspected relays and contactors visually inspected for leaks cleaned all pre filter media and \n",
    "replaced if applicable brushed coil fins as needed inspected blower wheels and fans lubricated motors and bearings as needed checked controls calibration and \n",
    "operation walk in tasks inspecfted all hinges and gaskets checked electrical contacts controls and components inspected relays and contactors visually inspected \n",
    "for leaks cleane dall pre filter media and replaced if applicable brushed coil fins as needed inspected blower wheels and fans lubricated motors and bearings \n",
    "inspected sight glass checked control calibration and operation ice machine tasks checked ice production thickness and sensing probes inspected door and gasket \n",
    "visually inspected bin and bin tstat inspected water pumps and distribution tubes visually inspected water filters inspected for any leaks cleaned condenser coils \n",
    "inspected all electrical components inspected fan motor  blades   bearings   lubricated as needed descaled ice machine exhaust fan tasks inspected roof fan belts and \n",
    "sheaves adjusted as needed replaced belts as needed  20x25x2 pleated filter high efficiency                                                                                                                                                       2023 Today  I arrived on site at unit IM01A and found that it was not operating  After troubleshooting  I discovered the following issues     Unit was off with a low water alarm    Water fill valve needs to be replaced   To resolve these issues  the following work was performed     Replaced the water fill valve    Replaced the water regulator    Checked over all operations and monitored the system   I am happy to report that the unit is now working normally  Upon departure  the unit was operating as expected  Solenoid  Water Feed 115V\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Perform POS tagging\n",
    "tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "# Initialize the WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to convert POS tag to WordNet POS tag\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Extract verbs and their corresponding nouns from the POS tagged tokens\n",
    "action_verbs = set()\n",
    "verb_noun_pairs = []\n",
    "for i, (word, pos) in enumerate(tagged_tokens):\n",
    "    wordnet_pos = get_wordnet_pos(pos) or wordnet.NOUN\n",
    "    lemma = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
    "    if wordnet_pos == wordnet.VERB:\n",
    "        action_verbs.add(lemma)\n",
    "        # Look for the corresponding noun before the verb\n",
    "        if i > 0 and tagged_tokens[i-1][1].startswith('NN'):\n",
    "            verb_noun_pairs.append((tagged_tokens[i-1][0], lemma))\n",
    "\n",
    "# Verify that all values in action_verbs are actually verbs using nltk\n",
    "def is_verb(word):\n",
    "    synsets = wordnet.synsets(word, pos=wordnet.VERB)\n",
    "    return bool(synsets)\n",
    "\n",
    "# Filter out non-verbs\n",
    "verified_action_verbs = [verb for verb in action_verbs if is_verb(verb)]\n",
    "\n",
    "# Combine verified action verbs with corresponding nouns\n",
    "verified_verb_noun_pairs = [(verb, noun) for verb,noun in verb_noun_pairs if verb in verified_action_verbs]\n",
    "\n",
    "# Print the identified action verbs\n",
    "print(\"Identified action verbs:\")\n",
    "print(list(action_verbs))\n",
    "\n",
    "# Print the verified action verbs\n",
    "print(\"Verified action verbs:\")\n",
    "print(verified_action_verbs)\n",
    "\n",
    "# Print the verified noun-verb pairs\n",
    "print(\"Verified noun-verb pairs:\")\n",
    "print(verified_noun_verb_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified action verbs:\n",
      "['arrive', 'discover', 'troubleshoot', 'sense', 'adjust', 'operate', 'Checked', 'Feed', 'work', 'Replaced', 'lubricate', 'exhaust', 'be', 'need', 'cleane', 'inspecfted', 'monitor', 'perform', 'walk', 'check', 'brush', 'replace', 'clean', 'report', 'resolve', 'test', 'expect', 'inspect', 'find']\n",
      "Verified action verbs:\n",
      "['arrive', 'discover', 'troubleshoot', 'sense', 'adjust', 'operate', 'Checked', 'Feed', 'work', 'Replaced', 'lubricate', 'exhaust', 'be', 'need', 'monitor', 'perform', 'walk', 'check', 'brush', 'replace', 'clean', 'report', 'resolve', 'test', 'expect', 'inspect', 'find']\n",
      "Verified verb-noun pairs:\n",
      "[('replace', 'air'), ('need', 'shut'), ('check', 'contactors'), ('inspect', 'evaporator'), ('test', 'heating'), ('inspect', 'relays'), ('brush', 'coil'), ('lubricate', 'motors'), ('need', 'checked'), ('inspect', 'relays'), ('brush', 'coil'), ('lubricate', 'motors'), ('check', 'control'), ('sense', 'probes'), ('inspect', 'door'), ('inspect', 'bin'), ('exhaust', 'fan'), ('need', 'replaced'), ('perform', 'Replaced'), ('expect', 'Solenoid')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Define the text to analyze\n",
    "text = \"\"\"\n",
    "performed pm per scope hvac tasks replaced air filters visually inspected for refrigerant leaks lubricated all bearings as needed shut off water supply  \n",
    "fall visit as needed  inspected belts and adjusted as needed cleaned condensate lines checked contactors visually inspected evaporator and condenser coils \n",
    "inspected and tested heating circuit for operation inspected and tested each condensate pump if applicable refrigerations tasks inspected all hinges and \n",
    "gaskets checked electrical contacts  controls and components inspected relays and contactors visually inspected for leaks cleaned all pre filter media and \n",
    "replaced if applicable brushed coil fins as needed inspected blower wheels and fans lubricated motors and bearings as needed checked controls calibration and \n",
    "operation walk in tasks inspecfted all hinges and gaskets checked electrical contacts controls and components inspected relays and contactors visually inspected \n",
    "for leaks cleane dall pre filter media and replaced if applicable brushed coil fins as needed inspected blower wheels and fans lubricated motors and bearings \n",
    "inspected sight glass checked control calibration and operation ice machine tasks checked ice production thickness and sensing probes inspected door and gasket \n",
    "visually inspected bin and bin tstat inspected water pumps and distribution tubes visually inspected water filters inspected for any leaks cleaned condenser coils \n",
    "inspected all electrical components inspected fan motor  blades   bearings   lubricated as needed descaled ice machine exhaust fan tasks inspected roof fan belts and \n",
    "sheaves adjusted as needed replaced belts as needed  20x25x2 pleated filter high efficiency                                                                                                                                                       2023 Today  I arrived on site at unit IM01A and found that it was not operating  After troubleshooting  I discovered the following issues     Unit was off with a low water alarm    Water fill valve needs to be replaced   To resolve these issues  the following work was performed     Replaced the water fill valve    Replaced the water regulator    Checked over all operations and monitored the system   I am happy to report that the unit is now working normally  Upon departure  the unit was operating as expected  Solenoid  Water Feed 115V\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Perform POS tagging\n",
    "tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "# Initialize the WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to convert POS tag to WordNet POS tag\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Extract verbs and their corresponding nouns from the POS tagged tokens\n",
    "action_verbs = set()\n",
    "verb_noun_pairs = []\n",
    "for i, (word, pos) in enumerate(tagged_tokens):\n",
    "    wordnet_pos = get_wordnet_pos(pos) or wordnet.NOUN\n",
    "    lemma = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
    "    if wordnet_pos == wordnet.VERB:\n",
    "        action_verbs.add(lemma)\n",
    "        # Look for the corresponding noun following the verb\n",
    "        if i < len(tagged_tokens) - 1 and tagged_tokens[i+1][1].startswith('NN'):\n",
    "            verb_noun_pairs.append((lemma, tagged_tokens[i+1][0]))\n",
    "\n",
    "# Verify that all values in action_verbs are actually verbs using nltk\n",
    "def is_verb(word):\n",
    "    synsets = wordnet.synsets(word, pos=wordnet.VERB)\n",
    "    return bool(synsets)\n",
    "\n",
    "# Filter out non-verbs\n",
    "verified_action_verbs = [verb for verb in action_verbs if is_verb(verb)]\n",
    "\n",
    "# Combine verified action verbs with corresponding nouns\n",
    "verified_verb_noun_pairs = [(verb, noun) for verb, noun in verb_noun_pairs if verb in verified_action_verbs]\n",
    "\n",
    "# Print the identified action verbs\n",
    "print(\"Identified action verbs:\")\n",
    "print(list(action_verbs))\n",
    "\n",
    "# Print the verified action verbs\n",
    "print(\"Verified action verbs:\")\n",
    "print(verified_action_verbs)\n",
    "\n",
    "# Print the verified verb-noun pairs\n",
    "print(\"Verified verb-noun pairs:\")\n",
    "print(verified_verb_noun_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
