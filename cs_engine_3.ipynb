{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maintenance Report NLP Analysis\n",
    "\n",
    "This notebook implements an NLP pipeline for analyzing maintenance reports using BERT and NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForTokenClassification, pipeline, logging\n",
    "from spellchecker import SpellChecker\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Suppress warnings from the transformers library\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "\n",
    "try:\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "except LookupError:\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS backend is available. PyTorch is using the GPU.\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "if platform.system() == 'Darwin':  # macOS\n",
    "    if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"MPS backend is available. PyTorch is using the GPU.\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"MPS backend is not available. PyTorch is using the CPU.\")\n",
    "else:  # Windows or other platforms\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"CUDA backend is available. PyTorch is using the GPU.\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"CUDA backend is not available. PyTorch is using the CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame loaded from data/ice_makers.csv\n",
      "(296547, 5)\n"
     ]
    }
   ],
   "source": [
    "# Step: Load Data\n",
    "\n",
    "# Define the path to ice makers' data file\n",
    "csv_file_path = 'data/ice_makers.csv'\n",
    "\n",
    "# Check if the data file exists\n",
    "if os.path.exists(csv_file_path):\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    print(f\"DataFrame loaded from {csv_file_path}\")\n",
    "else:\n",
    "    # Define the connection string\n",
    "    conn_str = (\n",
    "    r'DRIVER={ODBC Driver 17 for SQL Server};'\n",
    "    r'SERVER=35.184.99.218;'\n",
    "    r'DATABASE=coolsys;'\n",
    "    r'UID=sqlserver;'\n",
    "    r'PWD=Ybz8Vq+|>\\H/<2py'\n",
    "    )\n",
    "\n",
    "    # Define the SQL query\n",
    "    sql_query = \"\"\"\n",
    "    SELECT\n",
    "        w.wrkordr_wrk_rqstd,\n",
    "        w.wrkordr_wrk_prfrmd,\n",
    "        w2.wrkordreqpmnt_wrk_rqstd,\n",
    "        w2.wrkordreqpmnt_wrk_prfrmd,\n",
    "        w3.wrkordrinvntry_dscrptn\n",
    "    FROM\n",
    "        coolsys.dbo.wrkordr w\n",
    "    INNER JOIN coolsys.dbo.wrkordrinvntry w3 ON\n",
    "        w.wrkordr_rn = w3.wrkordr_rn\n",
    "    INNER JOIN coolsys.dbo.wrkordreqpmnt w2 ON\n",
    "        w.wrkordr_rn = w2.wrkordr_rn\n",
    "    WHERE\n",
    "        w.wrkordr_wrk_rqstd LIKE '%ICE MACHINE%' OR\n",
    "        w.wrkordr_wrk_prfrmd LIKE '%ICE MACHINE%' OR\n",
    "        w2.wrkordreqpmnt_wrk_rqstd LIKE '%ICE MACHINE%' OR\n",
    "        w2.wrkordreqpmnt_wrk_prfrmd LIKE '%ICE MACHINE%' OR\n",
    "        w3.wrkordrinvntry_dscrptn LIKE '%ICE MACHINE%';\n",
    "    \"\"\"\n",
    "\n",
    "    # Connect to the remote MSSQL database\n",
    "    conn = pyodbc.connect(conn_str)\n",
    "    print(\"Connection to the database was successful.\")\n",
    "\n",
    "    # Create a cursor object to execute SQL queries\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Execute the SQL query to retrieve all tables and columns\n",
    "    cursor.execute(sql_query)\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    # Get column names from the cursor description\n",
    "    columns = [column[0] for column in cursor.description]\n",
    "\n",
    "    if conn:\n",
    "        # Close the connection\n",
    "        conn.close()\n",
    "        print(\"Connection closed.\")\n",
    "\n",
    "    # Convert the fetched data to a pandas DataFrame\n",
    "    df = pd.DataFrame.from_records(rows, columns=columns)\n",
    "\n",
    "    # Export the DataFrame to a CSV file\n",
    "    df.to_csv(csv_file_path, index=False)\n",
    "    print(f\"DataFrame has been exported to {csv_file_path}\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(296547, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wrkordr_wrk_rqstd</th>\n",
       "      <th>wrkordr_wrk_prfrmd</th>\n",
       "      <th>wrkordreqpmnt_wrk_rqstd</th>\n",
       "      <th>wrkordreqpmnt_wrk_prfrmd</th>\n",
       "      <th>wrkordrinvntry_dscrptn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PREVENTIVE MAINT  --  2012 JAN DEEP DIVE\\rFILL...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PREVENTIVE MAINT  --  2012 JAN DEEP DIVE\\rFILL...</td>\n",
       "      <td>ST713886-PERFORMED PER SCOPE-NO PROBLEMS NOTED</td>\n",
       "      <td>ICE MACHINE CLEANER NICKEL SAFE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PREVENTIVE MAINT  --  2012 JAN DEEP DIVE\\rFILL...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PREVENTIVE MAINT  --  2012 JAN DEEP DIVE\\rFILL...</td>\n",
       "      <td>ST713890 - PERFORMED PM PER SCOPE</td>\n",
       "      <td>ICE MACHINE CLEANER NICKEL SAFE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SERVICE CALL  --  2012 JAN DEEP DIVE\\rFILL OUT...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SERVICE CALL  --  2012 JAN DEEP DIVE\\rFILL OUT...</td>\n",
       "      <td>ST713888-PERFORMED PER SCOPE-NO PROBLEMS NOTED</td>\n",
       "      <td>ICE MACHINE CLEANER NICKEL SAFE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PREVENTIVE MAINT  --  2012 JAN DEEP DIVE\\rFILL...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PREVENTIVE MAINT  --  2012 JAN DEEP DIVE\\rFILL...</td>\n",
       "      <td>COMPLETED PER SCOPE - NO PROBLEMS NOTED - CMP ...</td>\n",
       "      <td>ICE MACHINE CLEANER NICKEL SAFE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ICE MACHINE NOT WORKING - MAKING LOUD NOISE WH...</td>\n",
       "      <td>I/M R/R COND FAN MOTOR</td>\n",
       "      <td>ICE MACHINE NOT WORKING - MAKING LOUD NOISE WH...</td>\n",
       "      <td>ST 212897 - REMOVED AND REPLACED COND FAN MOTO...</td>\n",
       "      <td>FAN MTR 240V 606/806/1006 3/4 MS X MS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   wrkordr_wrk_rqstd      wrkordr_wrk_prfrmd  \\\n",
       "0  PREVENTIVE MAINT  --  2012 JAN DEEP DIVE\\rFILL...                     NaN   \n",
       "1  PREVENTIVE MAINT  --  2012 JAN DEEP DIVE\\rFILL...                     NaN   \n",
       "2  SERVICE CALL  --  2012 JAN DEEP DIVE\\rFILL OUT...                     NaN   \n",
       "3  PREVENTIVE MAINT  --  2012 JAN DEEP DIVE\\rFILL...                     NaN   \n",
       "4  ICE MACHINE NOT WORKING - MAKING LOUD NOISE WH...  I/M R/R COND FAN MOTOR   \n",
       "\n",
       "                             wrkordreqpmnt_wrk_rqstd  \\\n",
       "0  PREVENTIVE MAINT  --  2012 JAN DEEP DIVE\\rFILL...   \n",
       "1  PREVENTIVE MAINT  --  2012 JAN DEEP DIVE\\rFILL...   \n",
       "2  SERVICE CALL  --  2012 JAN DEEP DIVE\\rFILL OUT...   \n",
       "3  PREVENTIVE MAINT  --  2012 JAN DEEP DIVE\\rFILL...   \n",
       "4  ICE MACHINE NOT WORKING - MAKING LOUD NOISE WH...   \n",
       "\n",
       "                            wrkordreqpmnt_wrk_prfrmd  \\\n",
       "0     ST713886-PERFORMED PER SCOPE-NO PROBLEMS NOTED   \n",
       "1                  ST713890 - PERFORMED PM PER SCOPE   \n",
       "2     ST713888-PERFORMED PER SCOPE-NO PROBLEMS NOTED   \n",
       "3  COMPLETED PER SCOPE - NO PROBLEMS NOTED - CMP ...   \n",
       "4  ST 212897 - REMOVED AND REPLACED COND FAN MOTO...   \n",
       "\n",
       "                  wrkordrinvntry_dscrptn  \n",
       "0        ICE MACHINE CLEANER NICKEL SAFE  \n",
       "1        ICE MACHINE CLEANER NICKEL SAFE  \n",
       "2        ICE MACHINE CLEANER NICKEL SAFE  \n",
       "3        ICE MACHINE CLEANER NICKEL SAFE  \n",
       "4  FAN MTR 240V 606/806/1006 3/4 MS X MS  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(142141, 5)\n"
     ]
    }
   ],
   "source": [
    "# Define the strings to exclude from the DataFrame\n",
    "exclude_strings = [\n",
    "    'PREVENTIVE MAINT', 'PM SERVICE', \n",
    "    'Q1 MAINTENANCE', 'Q2 MAINTENANCE', 'Q3 MAINTENANCE','Q4 MAINTENANCE', \n",
    "    'NICKEL SAFE', 'COVID 19', 'SANITIZER'\n",
    "]\n",
    "\n",
    "\n",
    "# Create a boolean mask to identify rows where any column contains any of the search phrases\n",
    "mask = df.apply(lambda row: any(phrase in str(x) for phrase in exclude_strings for x in row), axis=1)\n",
    "\n",
    "# Filter the DataFrame to remove rows where any of the search phrases are present\n",
    "df_filtered = df[~mask]\n",
    "\n",
    "# Replace NaN values with empty strings\n",
    "# df_filtered = df_filtered.fillna('')\n",
    "\n",
    "print(df_filtered.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the records from the specified columns into 'combined_column' without changing the original columns\n",
    "df_filtered = df_filtered.copy()  # Create a copy to avoid SettingWithCopyWarning\n",
    "df_filtered.loc[:, 'combined_column'] = df_filtered.apply(\n",
    "    lambda row: ' '.join([str(row['wrkordr_wrk_prfrmd']) if pd.notna(row['wrkordr_wrk_prfrmd']) else '',\n",
    "                          str(row['wrkordreqpmnt_wrk_prfrmd']) if pd.notna(row['wrkordreqpmnt_wrk_prfrmd']) else '',\n",
    "                          str(row['wrkordrinvntry_dscrptn']) if pd.notna(row['wrkordrinvntry_dscrptn']) else '']),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace non-alphanumeric characters with a space using .loc\n",
    "df_filtered.loc[:, 'combined_column'] = df_filtered['combined_column'].str.replace(r'[^a-zA-Z0-9\\s]', ' ', regex=True)\n",
    "\n",
    "\n",
    "# Replace carriage return characters with a space using .loc\n",
    "df_filtered.loc[:, 'combined_column'] = df_filtered['combined_column'].str.replace(r'\\r', ' ', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wrkordr_wrk_rqstd</th>\n",
       "      <th>wrkordr_wrk_prfrmd</th>\n",
       "      <th>wrkordreqpmnt_wrk_rqstd</th>\n",
       "      <th>wrkordreqpmnt_wrk_prfrmd</th>\n",
       "      <th>wrkordrinvntry_dscrptn</th>\n",
       "      <th>combined_column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ICE MACHINE NOT WORKING - MAKING LOUD NOISE WH...</td>\n",
       "      <td>I/M R/R COND FAN MOTOR</td>\n",
       "      <td>ICE MACHINE NOT WORKING - MAKING LOUD NOISE WH...</td>\n",
       "      <td>ST 212897 - REMOVED AND REPLACED COND FAN MOTO...</td>\n",
       "      <td>FAN MTR 240V 606/806/1006 3/4 MS X MS</td>\n",
       "      <td>I M R R COND FAN MOTOR ST 212897   REMOVED AND...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ICE MACHINE NOT WORKING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ICE MACHINE NOT WORKING</td>\n",
       "      <td>INSTALLED 9 LBS R404 IN ICE MACHINE - UNIT IS ...</td>\n",
       "      <td>R404A FREON 24 LBS LT &amp; MT HFC HP62</td>\n",
       "      <td>INSTALLED 9 LBS R404 IN ICE MACHINE   UNIT IS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ICE MACHINE NOT WORKING -- ERVICE CALL  --  I/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ICE MACHINE NOT WORKING -- ERVICE CALL  --  I/...</td>\n",
       "      <td>EVAP HOT WAS BEEPING AND CUSTOMER RESET -- TRA...</td>\n",
       "      <td>R404A FREON 24 LBS LT &amp; MT HFC HP62</td>\n",
       "      <td>EVAP HOT WAS BEEPING AND CUSTOMER RESET    TR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ICE MACHINE NOT WORKING -- REDISPATCH  --  ICE...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ICE MACHINE NOT WORKING -- REDISPATCH  --  ICE...</td>\n",
       "      <td>FOUND DIRECT SHORT BETWEEEN CONDENSER AND FAN ...</td>\n",
       "      <td>RELAY 2 POLE 240V</td>\n",
       "      <td>FOUND DIRECT SHORT BETWEEEN CONDENSER AND FAN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ICE MACHINE NOT WORKING -- SERVICE CALL  --  I...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ICE MACHINE NOT WORKING -- SERVICE CALL  --  I...</td>\n",
       "      <td>FOUND THAT GAS VALVE LEAKING--NEED R/R IN AM D...</td>\n",
       "      <td>R404A FREON 24 LBS LT &amp; MT HFC HP62</td>\n",
       "      <td>FOUND THAT GAS VALVE LEAKING  NEED R R IN AM ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   wrkordr_wrk_rqstd      wrkordr_wrk_prfrmd  \\\n",
       "4  ICE MACHINE NOT WORKING - MAKING LOUD NOISE WH...  I/M R/R COND FAN MOTOR   \n",
       "5                            ICE MACHINE NOT WORKING                     NaN   \n",
       "6  ICE MACHINE NOT WORKING -- ERVICE CALL  --  I/...                     NaN   \n",
       "7  ICE MACHINE NOT WORKING -- REDISPATCH  --  ICE...                     NaN   \n",
       "8  ICE MACHINE NOT WORKING -- SERVICE CALL  --  I...                     NaN   \n",
       "\n",
       "                             wrkordreqpmnt_wrk_rqstd  \\\n",
       "4  ICE MACHINE NOT WORKING - MAKING LOUD NOISE WH...   \n",
       "5                            ICE MACHINE NOT WORKING   \n",
       "6  ICE MACHINE NOT WORKING -- ERVICE CALL  --  I/...   \n",
       "7  ICE MACHINE NOT WORKING -- REDISPATCH  --  ICE...   \n",
       "8  ICE MACHINE NOT WORKING -- SERVICE CALL  --  I...   \n",
       "\n",
       "                            wrkordreqpmnt_wrk_prfrmd  \\\n",
       "4  ST 212897 - REMOVED AND REPLACED COND FAN MOTO...   \n",
       "5  INSTALLED 9 LBS R404 IN ICE MACHINE - UNIT IS ...   \n",
       "6  EVAP HOT WAS BEEPING AND CUSTOMER RESET -- TRA...   \n",
       "7  FOUND DIRECT SHORT BETWEEEN CONDENSER AND FAN ...   \n",
       "8  FOUND THAT GAS VALVE LEAKING--NEED R/R IN AM D...   \n",
       "\n",
       "                  wrkordrinvntry_dscrptn  \\\n",
       "4  FAN MTR 240V 606/806/1006 3/4 MS X MS   \n",
       "5    R404A FREON 24 LBS LT & MT HFC HP62   \n",
       "6    R404A FREON 24 LBS LT & MT HFC HP62   \n",
       "7                      RELAY 2 POLE 240V   \n",
       "8    R404A FREON 24 LBS LT & MT HFC HP62   \n",
       "\n",
       "                                     combined_column  \n",
       "4  I M R R COND FAN MOTOR ST 212897   REMOVED AND...  \n",
       "5   INSTALLED 9 LBS R404 IN ICE MACHINE   UNIT IS...  \n",
       "6   EVAP HOT WAS BEEPING AND CUSTOMER RESET    TR...  \n",
       "7   FOUND DIRECT SHORT BETWEEEN CONDENSER AND FAN...  \n",
       "8   FOUND THAT GAS VALVE LEAKING  NEED R R IN AM ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the filtered DataFrame\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random sample of 1000 records from the DataFrame\n",
    "df_filtered = df_filtered.sample(n=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tasks_by_category, category_totals, task_frequencies, all_verb_object_pairs\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Process the data with progress bar\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m tasks_by_category, category_totals, task_frequencies, all_pairs \u001b[38;5;241m=\u001b[39m process_maintenance_data(\u001b[43mdf_sample\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_sample' is not defined"
     ]
    }
   ],
   "source": [
    "# Import required library for progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "def process_maintenance_data(df):\n",
    "    \"\"\"\n",
    "    Process maintenance data with progress bar visualization\n",
    "    \"\"\"\n",
    "    tasks_by_category = defaultdict(list)\n",
    "    category_totals = defaultdict(int)\n",
    "    task_frequencies = defaultdict(int)\n",
    "    all_verb_object_pairs = []\n",
    "    \n",
    "    print(\"Starting maintenance data processing...\")\n",
    "    total_records = len(df['combined_column'].dropna())\n",
    "    print(f\"Total records to process: {total_records}\")\n",
    "    \n",
    "    # Process records with progress bar\n",
    "    with tqdm(total=total_records, desc='Processing Records', unit='record') as pbar:\n",
    "        for text in df['combined_column'].dropna():\n",
    "            # Process the text\n",
    "            pairs = process_text(str(text), tokenizer, model, device)\n",
    "            \n",
    "            if pairs:\n",
    "                for verb, obj in pairs:\n",
    "                    category = categorize_task(verb, obj)\n",
    "                    task = f\"{verb}: {obj}\"\n",
    "                    \n",
    "                    # Update frequencies and store pairs\n",
    "                    task_frequencies[task] += 1\n",
    "                    category_totals[category] += 1\n",
    "                    all_verb_object_pairs.append((verb, obj))\n",
    "                    \n",
    "                    # Check if task already exists in category\n",
    "                    task_exists = False\n",
    "                    for existing_task in tasks_by_category[category]:\n",
    "                        if existing_task[0] == task:\n",
    "                            task_exists = True\n",
    "                            break\n",
    "                            \n",
    "                    if not task_exists:\n",
    "                        tasks_by_category[category].append(\n",
    "                            (task, verb, task_frequencies[task])\n",
    "                        )\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.update(1)\n",
    "    \n",
    "    print(f\"\\nProcessing complete!\")\n",
    "    print(f\"Total task occurrences: {len(all_verb_object_pairs)}\")\n",
    "    print(f\"Unique tasks: {len(task_frequencies)}\")\n",
    "    \n",
    "    return tasks_by_category, category_totals, task_frequencies, all_verb_object_pairs\n",
    "\n",
    "# Process the data with progress bar\n",
    "tasks_by_category, category_totals, task_frequencies, all_pairs = process_maintenance_data(df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    try:\n",
    "        # Check if GPU/MPS is available and set the device\n",
    "        if torch.backends.mps.is_available():\n",
    "            device = torch.device(\"mps\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "        \n",
    "        # Load the pre-trained BERT model for POS tagging\n",
    "        model_name = 'vblagoje/bert-english-uncased-finetuned-pos'\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        model = BertForTokenClassification.from_pretrained(model_name).to(device)\n",
    "        \n",
    "        return tokenizer, model, device\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing model: {str(e)}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Initialize the model\n",
    "tokenizer, model, device = initialize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text, tokenizer, model, device):\n",
    "    try:\n",
    "        # Text preprocessing\n",
    "        text = re.sub(r'\\bas needed\\b', '', text.lower())\n",
    "        text = re.sub(r'\\s*,\\s*and\\s+', ', ', text)  # Normalize conjunctions\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        # Initialize the spell checker and lemmatizer\n",
    "        spell = SpellChecker()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        # Process the text\n",
    "        tokens = word_tokenize(text)\n",
    "        tagged = pos_tag(tokens)\n",
    "        \n",
    "        # Extract verbs and their corresponding objects\n",
    "        verb_object_pairs = []\n",
    "        current_verb = None\n",
    "        current_objects = []\n",
    "        \n",
    "        def is_potential_verb(word, tag):\n",
    "            # Check if word ends with common past tense/participle endings\n",
    "            return (tag.startswith('VB') or tag == 'VBD' or tag == 'VBN' or \n",
    "                   (tag == 'JJ' and (word.endswith('ed') or word.endswith('en'))))\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(tagged):\n",
    "            word, tag = tagged[i]\n",
    "            \n",
    "            # Handle verbs (including past tense, participles, and adjectives that are actually verbs)\n",
    "            if is_potential_verb(word, tag):\n",
    "                # Save previous pair if exists\n",
    "                if current_verb and current_objects:\n",
    "                    verb_object_pairs.append((lemmatizer.lemmatize(current_verb, 'v'), ' '.join(current_objects)))\n",
    "                current_verb = spell.correction(word)\n",
    "                current_objects = []\n",
    "            \n",
    "            # Handle nouns, adjectives, and compound objects\n",
    "            elif (tag.startswith('NN') or \n",
    "                  (tag.startswith('JJ') and not word.endswith('ed')) or \n",
    "                  tag == 'IN' or tag.startswith('VBG')):  # Include prepositions and gerunds\n",
    "                if current_verb:\n",
    "                    temp_objects = [word]\n",
    "                    # Look ahead for compound objects and their modifiers\n",
    "                    j = i + 1\n",
    "                    while j < len(tagged) and (\n",
    "                        tagged[j][1].startswith('NN') or \n",
    "                        (tagged[j][1].startswith('JJ') and not tagged[j][0].endswith('ed')) or \n",
    "                        tagged[j][1] == 'IN' or  # Include prepositions\n",
    "                        tagged[j][1].startswith('VBG')  # Include gerunds\n",
    "                    ):\n",
    "                        temp_objects.append(tagged[j][0])\n",
    "                        j += 1\n",
    "                    \n",
    "                    # Only add if we have a meaningful object phrase\n",
    "                    if any(t[1].startswith('NN') for t in tagged[i:j]):\n",
    "                        current_objects.extend(temp_objects)\n",
    "                    i = j - 1  # Update index to skip processed compound words\n",
    "            \n",
    "            # Handle commas as phrase separators\n",
    "            elif word == ',':\n",
    "                if current_verb and current_objects:\n",
    "                    verb_object_pairs.append((lemmatizer.lemmatize(current_verb, 'v'), ' '.join(current_objects)))\n",
    "                    current_objects = []\n",
    "                    current_verb = None\n",
    "            \n",
    "            i += 1\n",
    "\n",
    "        # Add the last pair if exists\n",
    "        if current_verb and current_objects:\n",
    "            verb_object_pairs.append((lemmatizer.lemmatize(current_verb, 'v'), ' '.join(current_objects)))\n",
    "\n",
    "        return verb_object_pairs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_task(verb, obj):\n",
    "    \"\"\"Categorize a task based on its verb into one of four categories: Inspection, Cleaning, Maintenance, or Other.\"\"\"\n",
    "    inspection_verbs = {'inspect', 'check', 'examine', 'monitor', 'observe', 'verify', 'test', 'diagnose', 'assess'}\n",
    "    cleaning_verbs = {'clean', 'brush', 'wash', 'wipe', 'descale', 'sanitize', 'flush', 'clear'}\n",
    "    maintenance_verbs = {'lubricate', 'adjust', 'replace', 'repair', 'calibrate', 'install', 'fix', 'tighten', 'service'}\n",
    "    \n",
    "    verb = verb.lower()\n",
    "    if verb in inspection_verbs:\n",
    "        return \"Inspection\"\n",
    "    elif verb in cleaning_verbs:\n",
    "        return \"Cleaning\"\n",
    "    elif verb in maintenance_verbs:\n",
    "        return \"Maintenance\"\n",
    "    else:\n",
    "        return \"Other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to process maintenance records...\n",
      "Total records to process: 10000\n",
      "Processing record 100/10000 (1.0%)\n",
      "Processing record 200/10000 (2.0%)\n",
      "Processing record 300/10000 (3.0%)\n",
      "Processing record 400/10000 (4.0%)\n",
      "Processing record 500/10000 (5.0%)\n",
      "Processing record 600/10000 (6.0%)\n",
      "Processing record 700/10000 (7.0%)\n",
      "Processing record 800/10000 (8.0%)\n",
      "Processing record 900/10000 (9.0%)\n",
      "Processing record 1000/10000 (10.0%)\n",
      "Processing record 1100/10000 (11.0%)\n",
      "Processing record 1200/10000 (12.0%)\n",
      "Processing record 1300/10000 (13.0%)\n"
     ]
    }
   ],
   "source": [
    "# Process all records in df_filtered['combined_column']\n",
    "from collections import Counter\n",
    "\n",
    "print(\"Starting to process maintenance records...\")\n",
    "\n",
    "# Initialize counters for tasks and categories\n",
    "all_verb_object_pairs = []  # Keep as list to track frequencies\n",
    "task_frequencies = Counter()\n",
    "\n",
    "# Process each record\n",
    "total_records = len(df_filtered['combined_column'].dropna())\n",
    "print(f\"Total records to process: {total_records}\")\n",
    "\n",
    "for idx, text in enumerate(df_filtered['combined_column'].dropna(), 1):\n",
    "    if idx % 100 == 0:  # Print progress every 100 records\n",
    "        print(f\"Processing record {idx}/{total_records} ({(idx/total_records*100):.1f}%)\")\n",
    "    \n",
    "    pairs = process_text(text, tokenizer, model, device)\n",
    "    if pairs:  # Only add if we got valid pairs\n",
    "        for pair in pairs:\n",
    "            task_frequencies[tuple(pair)] += 1\n",
    "            all_verb_object_pairs.append(tuple(pair))\n",
    "\n",
    "print(f\"\\nTotal task occurrences: {len(all_verb_object_pairs)}\")\n",
    "print(f\"Unique tasks: {len(task_frequencies)}\")\n",
    "\n",
    "# Group tasks by category with frequencies\n",
    "tasks_by_category = {}\n",
    "category_totals = {}\n",
    "\n",
    "for (verb, obj), freq in task_frequencies.most_common():\n",
    "    category = categorize_task(verb, obj)\n",
    "    if category not in tasks_by_category:\n",
    "        tasks_by_category[category] = []\n",
    "        category_totals[category] = 0\n",
    "    tasks_by_category[category].append((verb, obj, freq))\n",
    "    category_totals[category] += freq\n",
    "\n",
    "# Print tasks by category with frequencies\n",
    "print(\"\\nTask Categories Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "total_tasks = sum(category_totals.values())\n",
    "\n",
    "for category in [\"Other\", \"Maintenance\", \"Inspection\", \"Cleaning\"]:\n",
    "    if category in tasks_by_category:\n",
    "        print(f\"\\n{category} Tasks ({category_totals[category]} occurrences, {(category_totals[category]/total_tasks*100):.1f}%)\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Sort tasks by frequency within category\n",
    "        sorted_tasks = sorted(tasks_by_category[category], key=lambda x: (-x[2], x[0]))\n",
    "        \n",
    "        for verb, obj, freq in sorted_tasks:\n",
    "            percentage = (freq / category_totals[category]) * 100\n",
    "            print(f\"â€¢ {verb.capitalize()}: {obj} ({freq} times, {percentage:.1f}% of {category})\")\n",
    "\n",
    "# Print overall statistics\n",
    "print(\"\\nTask Distribution Statistics\")\n",
    "print(\"=\" * 50)\n",
    "for category, total in category_totals.items():\n",
    "    percentage = (total / total_tasks) * 100\n",
    "    unique_tasks = len(tasks_by_category[category])\n",
    "    print(f\"{category}: {total} occurrences ({percentage:.1f}%), {unique_tasks} unique tasks\")\n",
    "\n",
    "# Create a summary DataFrame\n",
    "summary_data = {\n",
    "    'Category': [],\n",
    "    'Total_Occurrences': [],\n",
    "    'Unique_Tasks': [],\n",
    "    'Percentage': []\n",
    "}\n",
    "\n",
    "for category in tasks_by_category:\n",
    "    summary_data['Category'].append(category)\n",
    "    summary_data['Total_Occurrences'].append(category_totals[category])\n",
    "    summary_data['Unique_Tasks'].append(len(tasks_by_category[category]))\n",
    "    summary_data['Percentage'].append((category_totals[category] / total_tasks) * 100)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nSummary DataFrame:\")\n",
    "print(\"=\" * 50)\n",
    "print(summary_df.to_string(index=False, float_format=lambda x: '{:.1f}'.format(x)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coolsys_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
