{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maintenance Report NLP Analysis\n",
    "\n",
    "This notebook implements an NLP pipeline for analyzing maintenance reports using BERT and NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/Users/nikolay_tishchenko/nltk_data'\n    - '/Users/nikolay_tishchenko/codeium/coolsys/coolsys_env/nltk_data'\n    - '/Users/nikolay_tishchenko/codeium/coolsys/coolsys_env/share/nltk_data'\n    - '/Users/nikolay_tishchenko/codeium/coolsys/coolsys_env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Set up NLTK data\u001b[39;00m\n\u001b[1;32m     18\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcorpora/wordnet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# ... (rest of the code remains the same)\u001b[39;00m\n",
      "File \u001b[0;32m~/codeium/coolsys/coolsys_env/lib/python3.11/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/Users/nikolay_tishchenko/nltk_data'\n    - '/Users/nikolay_tishchenko/codeium/coolsys/coolsys_env/nltk_data'\n    - '/Users/nikolay_tishchenko/codeium/coolsys/coolsys_env/share/nltk_data'\n    - '/Users/nikolay_tishchenko/codeium/coolsys/coolsys_env/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import nltk\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Set up device (GPU, MPS, or CPU)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Set up NLTK data\n",
    "nltk.data.find(\"tokenizers/punkt\")\n",
    "nltk.data.find(\"corpora/wordnet\")\n",
    "\n",
    "# ... (rest of the code remains the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings from the transformers library\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"corpora/wordnet\")\n",
    "except LookupError:\n",
    "    nltk.download(\"wordnet\", quiet=True)\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"taggers/averaged_perceptron_tagger\")\n",
    "except LookupError:\n",
    "    nltk.download(\"averaged_perceptron_tagger\", quiet=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "import nltk\n",
    "import pyodbc\n",
    "import logging\n",
    "import re\n",
    "\n",
    "from typing import Optional, List, Tuple\n",
    "import time\n",
    "import os\n",
    "from spellchecker import SpellChecker\n",
    "from functools import lru_cache\n",
    "\n",
    "import platform\n",
    "\n",
    "from transformers import BertTokenizer, BertForTokenClassification, pipeline, logging\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from typing import Optional, List, Tuple\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Set up device (GPU, MPS, or CPU)\n",
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    torch.cuda.set_device(0)  # Set the GPU device to use\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Set up NLTK data\n",
    "import nltk\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "import nltk\n",
    "import pyodbc\n",
    "import logging\n",
    "import re\n",
    "\n",
    "from typing import Optional, List, Tuple\n",
    "import time\n",
    "import os\n",
    "from spellchecker import SpellChecker\n",
    "from functools import lru_cache\n",
    "\n",
    "import platform\n",
    "\n",
    "from transformers import BertTokenizer, BertForTokenClassification, pipeline, logging\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from typing import Optional, List, Tuple\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Set up device (GPU, MPS, or CPU)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    torch.cuda.set_device(0)  # Set the GPU device to use\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Set up NLTK data\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "import nltk\n",
    "import pyodbc\n",
    "import logging\n",
    "import re\n",
    "\n",
    "from typing import Optional, List, Tuple\n",
    "import time\n",
    "import os\n",
    "from spellchecker import SpellChecker\n",
    "from functools import lru_cache\n",
    "\n",
    "import platform\n",
    "\n",
    "from transformers import BertTokenizer, BertForTokenClassification, pipeline, logging\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from typing import Optional, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Set up device (GPU, MPS, or CPU)\n",
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    torch.cuda.set_device(0)  # Set the GPU device to use\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Set up NLTK data\n",
    "import nltk\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings from the transformers library\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"corpora/wordnet\")\n",
    "except LookupError:\n",
    "    nltk.download(\"wordnet\", quiet=True)\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"taggers/averaged_perceptron_tagger\")\n",
    "except LookupError:\n",
    "    nltk.download(\"averaged_perceptron_tagger\", quiet=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Constants\n",
    "CSV_FILE_PATH = 'data/ice_makers.csv'\n",
    "DB_CONFIG = {\n",
    "    'driver': 'ODBC Driver 17 for SQL Server',\n",
    "    'server': '35.184.99.218',\n",
    "    'database': 'coolsys',\n",
    "    'uid': 'sqlserver',\n",
    "    'pwd': 'Ybz8Vq+|>\\H/<2py'\n",
    "}\n",
    "\n",
    "SQL_QUERY = \"\"\"\n",
    "SELECT\n",
    "    w.wrkordr_wrk_rqstd,\n",
    "    w.wrkordr_wrk_prfrmd,\n",
    "    w2.wrkordreqpmnt_wrk_rqstd,\n",
    "    w2.wrkordreqpmnt_wrk_prfrmd,\n",
    "    w3.wrkordrinvntry_dscrptn\n",
    "FROM\n",
    "    coolsys.dbo.wrkordr w\n",
    "    INNER JOIN coolsys.dbo.wrkordrinvntry w3 ON w.wrkordr_rn = w3.wrkordr_rn\n",
    "    INNER JOIN coolsys.dbo.wrkordreqpmnt w2 ON w.wrkordr_rn = w2.wrkordr_rn\n",
    "WHERE\n",
    "    w.wrkordr_wrk_rqstd LIKE '%ICE MACHINE%'\n",
    "    OR w.wrkordr_wrk_prfrmd LIKE '%ICE MACHINE%'\n",
    "    OR w2.wrkordreqpmnt_wrk_rqstd LIKE '%ICE MACHINE%'\n",
    "    OR w2.wrkordreqpmnt_wrk_prfrmd LIKE '%ICE MACHINE%'\n",
    "    OR w3.wrkordrinvntry_dscrptn LIKE '%ICE MACHINE%';\n",
    "\"\"\"\n",
    "\n",
    "@contextmanager\n",
    "def database_connection():\n",
    "    \"\"\"Context manager for database connections with proper error handling.\"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn_str = (\n",
    "            f'DRIVER={{{DB_CONFIG[\"driver\"]}}};'\n",
    "            f'SERVER={DB_CONFIG[\"server\"]};'\n",
    "            f'DATABASE={DB_CONFIG[\"database\"]};'\n",
    "            f'UID={DB_CONFIG[\"uid\"]};'\n",
    "            f'PWD={DB_CONFIG[\"pwd\"]}'\n",
    "        )\n",
    "        conn = pyodbc.connect(conn_str, timeout=30)  # Add connection timeout\n",
    "        logger.info(\"Database connection established\")\n",
    "        yield conn\n",
    "    except pyodbc.Error as e:\n",
    "        logger.error(f\"Database connection error: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "            logger.info(\"Database connection closed\")\n",
    "\n",
    "def fetch_data_from_db() -> Tuple[Optional[pd.DataFrame], Optional[str]]:\n",
    "    \"\"\"Fetch data from database with optimized performance and error handling.\"\"\"\n",
    "    try:\n",
    "        with database_connection() as conn:\n",
    "            # Configure connection for better performance\n",
    "            conn.setdecoding(pyodbc.SQL_CHAR, encoding='utf-8')\n",
    "            conn.setdecoding(pyodbc.SQL_WCHAR, encoding='utf-8')\n",
    "            conn.setencoding(encoding='utf-8')\n",
    "            \n",
    "            # Use pandas read_sql for better performance\n",
    "            start_time = time.time()\n",
    "            df = pd.read_sql(SQL_QUERY, conn)\n",
    "            logger.info(f\"Query executed in {time.time() - start_time:.2f} seconds\")\n",
    "            \n",
    "            return df, None\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error fetching data: {str(e)}\"\n",
    "        logger.error(error_msg, exc_info=True)\n",
    "        return None, error_msg\n",
    "\n",
    "def save_to_csv(df: pd.DataFrame, filepath: str) -> None:\n",
    "    \"\"\"Save DataFrame to CSV with error handling.\"\"\"\n",
    "    try:\n",
    "        # Ensure directory exists\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        \n",
    "        # Save with optimized settings\n",
    "        df.to_csv(filepath, index=False, compression='infer')\n",
    "        logger.info(f\"Data saved to {filepath}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving to CSV: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "def load_data() -> pd.DataFrame:\n",
    "    \"\"\"Main function to load data with caching mechanism.\"\"\"\n",
    "    try:\n",
    "        # Try loading from cache first\n",
    "        if os.path.exists(CSV_FILE_PATH):\n",
    "            logger.info(f\"Loading data from cache: {CSV_FILE_PATH}\")\n",
    "            return pd.read_csv(CSV_FILE_PATH)\n",
    "        \n",
    "        # Fetch from database if cache doesn't exist\n",
    "        logger.info(\"Cache not found, fetching from database\")\n",
    "        df, error = fetch_data_from_db()\n",
    "        \n",
    "        if error:\n",
    "            raise RuntimeError(error)\n",
    "        \n",
    "        if df is not None and not df.empty:\n",
    "            # Save to cache\n",
    "            save_to_csv(df, CSV_FILE_PATH)\n",
    "            return df\n",
    "        else:\n",
    "            raise ValueError(\"No data retrieved from database\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in load_data: {str(e)}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "# Main execution\n",
    "try:\n",
    "    df = load_data()\n",
    "    logger.info(f\"DataFrame shape: {df.shape}\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to load data\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Configure logging if not already configured\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define columns to combine\n",
    "columns_to_combine = ['wrkordr_wrk_prfrmd', 'wrkordreqpmnt_wrk_prfrmd', 'wrkordrinvntry_dscrptn']\n",
    "\n",
    "# Pre-compile the regex pattern for better performance\n",
    "pattern = re.compile(r'[^a-zA-Z0-9\\s]|\\r')\n",
    "\n",
    "try:\n",
    "    # Make a copy of only source columns\n",
    "    df = df[columns_to_combine].copy()\n",
    "\n",
    "    # Perform operations in a memory-efficient way\n",
    "    df['combined_column'] = (\n",
    "        df[columns_to_combine]\n",
    "        .fillna('')  # Replace NaN with empty string\n",
    "        .astype(str)  # Convert to string type (more compatible than string[pyarrow])\n",
    "        .agg(' '.join, axis=1)  # Join columns with space\n",
    "        .str.replace(pattern, ' ', regex=True)  # Clean text using pre-compiled pattern\n",
    "    )\n",
    "\n",
    "    # Clean up extra whitespace\n",
    "    df['combined_column'] = df['combined_column'].str.strip().str.replace(r'\\s+', ' ')\n",
    "\n",
    "    # Optional: Free memory if the original columns are no longer needed\n",
    "    if columns_to_combine:\n",
    "        df.drop(columns=columns_to_combine, inplace=True)\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error processing DataFrame: {str(e)}\")\n",
    "    raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random sample of 100,000 records from the DataFrame\n",
    "df_sample = df.sample(n=10000, random_state=42)\n",
    "# df_sample = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import torch\n",
    "from transformers import (\n",
    "    BertTokenizer, \n",
    "    BertForTokenClassification, \n",
    "    logging as transformers_logging,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification\n",
    ")\n",
    "import logging\n",
    "from typing import Tuple, Optional\n",
    "import os\n",
    "import sys\n",
    "from transformers import __version__ as transformers_version\n",
    "\n",
    "# Configure detailed logging\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,  # Changed to DEBUG for more detailed logs\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "transformers_logging.set_verbosity_info()\n",
    "\n",
    "def check_environment():\n",
    "    \"\"\"Check and log environment details.\"\"\"\n",
    "    logger.info(f\"Python version: {sys.version}\")\n",
    "    logger.info(f\"PyTorch version: {torch.__version__}\")\n",
    "    logger.info(f\"Transformers version: {transformers_version}\")\n",
    "    logger.info(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        logger.info(f\"CUDA version: {torch.version.cuda}\")\n",
    "        logger.info(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"Determine the optimal device with caching.\"\"\"\n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            logger.info(\"CUDA device detected\")\n",
    "            return torch.device(\"cuda\")\n",
    "        elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "            logger.info(\"MPS device detected\")\n",
    "            return torch.device(\"mps\")\n",
    "        logger.info(\"Using CPU device\")\n",
    "        return torch.device(\"cpu\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error detecting device, defaulting to CPU: {str(e)}\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "def load_model_components(model_name: str, device: torch.device) -> Tuple[Optional[BertTokenizer], Optional[BertForTokenClassification]]:\n",
    "    \"\"\"Load model components with optimizations and proper error handling.\"\"\"\n",
    "    try:\n",
    "        # Set up caching directory\n",
    "        cache_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"huggingface\")\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        logger.info(f\"Using cache directory: {cache_dir}\")\n",
    "\n",
    "        # Try loading with Auto classes first\n",
    "        logger.info(\"Attempting to load tokenizer...\")\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_name,\n",
    "                cache_dir=cache_dir,\n",
    "                use_fast=True,\n",
    "                model_max_length=512\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"AutoTokenizer failed, trying BertTokenizer: {str(e)}\")\n",
    "            tokenizer = BertTokenizer.from_pretrained(\n",
    "                model_name,\n",
    "                cache_dir=cache_dir,\n",
    "                use_fast=True,\n",
    "                model_max_length=512\n",
    "            )\n",
    "        logger.info(\"Tokenizer loaded successfully\")\n",
    "\n",
    "        # Load model with Auto class\n",
    "        logger.info(\"Attempting to load model...\")\n",
    "        try:\n",
    "            model = AutoModelForTokenClassification.from_pretrained(\n",
    "                model_name,\n",
    "                cache_dir=cache_dir,\n",
    "                return_dict=True,  # Changed to True for better compatibility\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"AutoModel failed, trying BertForTokenClassification: {str(e)}\")\n",
    "            model = BertForTokenClassification.from_pretrained(\n",
    "                model_name,\n",
    "                cache_dir=cache_dir,\n",
    "                return_dict=True,\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "        logger.info(\"Model loaded successfully\")\n",
    "\n",
    "        # Move model to device and optimize\n",
    "        logger.info(f\"Moving model to device: {device}\")\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        # Disable gradient computation for inference\n",
    "        torch.set_grad_enabled(False)\n",
    "        \n",
    "        return tokenizer, model\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model components: {str(e)}\", exc_info=True)\n",
    "        return None, None\n",
    "\n",
    "def initialize_model() -> Tuple[Optional[BertTokenizer], Optional[BertForTokenClassification], Optional[torch.device]]:\n",
    "    \"\"\"Initialize model with proper error handling and optimizations.\"\"\"\n",
    "    try:\n",
    "        # Check environment first\n",
    "        check_environment()\n",
    "        \n",
    "        # Get device (cached)\n",
    "        device = get_device()\n",
    "        logger.info(f\"Using device: {device}\")\n",
    "\n",
    "        # Try alternative model if the original fails\n",
    "        model_names = [\n",
    "            'vblagoje/bert-english-uncased-finetuned-pos',\n",
    "            'bert-base-uncased'  # Fallback model\n",
    "        ]\n",
    "        \n",
    "        for model_name in model_names:\n",
    "            logger.info(f\"Attempting to load model: {model_name}\")\n",
    "            tokenizer, model = load_model_components(model_name, device)\n",
    "            \n",
    "            if tokenizer is not None and model is not None:\n",
    "                logger.info(f\"Successfully loaded model: {model_name}\")\n",
    "                return tokenizer, model, device\n",
    "            \n",
    "            logger.warning(f\"Failed to load model: {model_name}, trying next option...\")\n",
    "        \n",
    "        raise RuntimeError(\"All model loading attempts failed\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error initializing model: {str(e)}\", exc_info=True)\n",
    "        return None, None, None\n",
    "\n",
    "# Initialize the model with proper error handling\n",
    "try:\n",
    "    # Clear any existing cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    tokenizer, model, device = initialize_model()\n",
    "    if None in (tokenizer, model, device):\n",
    "        raise RuntimeError(\"Model initialization failed\")\n",
    "    logger.info(\"Model initialized successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Failed to initialize model\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import re\n",
    "from typing import List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ProcessingContext:\n",
    "    current_verb: Optional[str] = None\n",
    "    current_objects: List[str] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.current_objects is None:\n",
    "            self.current_objects = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_verb = None\n",
    "        self.current_objects.clear()\n",
    "\n",
    "@lru_cache(maxsize=1024)\n",
    "def is_potential_verb(word: str, tag: str) -> bool:\n",
    "    \"\"\"Cached check for potential verbs.\"\"\"\n",
    "    return (tag.startswith('VB') or tag == 'VBD' or tag == 'VBN' or \n",
    "            (tag == 'JJ' and (word.endswith('ed') or word.endswith('en'))))\n",
    "\n",
    "@lru_cache(maxsize=1024)\n",
    "def is_object_component(tag: str, word: str) -> bool:\n",
    "    \"\"\"Cached check for object components.\"\"\"\n",
    "    return (tag.startswith('NN') or \n",
    "            (tag.startswith('JJ') and not word.endswith('ed')) or \n",
    "            tag == 'IN' or tag.startswith('VBG'))\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"Preprocess text with compiled regex patterns.\"\"\"\n",
    "    # Compile regex patterns once\n",
    "    PATTERNS = {\n",
    "        'as_needed': re.compile(r'\\bas needed\\b'),\n",
    "        'conjunctions': re.compile(r'\\s*,\\s*and\\s+'),\n",
    "        'whitespace': re.compile(r'\\s+')\n",
    "    }\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = PATTERNS['as_needed'].sub('', text)\n",
    "    text = PATTERNS['conjunctions'].sub(', ', text)\n",
    "    return PATTERNS['whitespace'].sub(' ', text).strip()\n",
    "\n",
    "def process_compound_objects(tagged: List[Tuple[str, str]], start_idx: int) -> Tuple[List[str], int]:\n",
    "    \"\"\"Process compound objects and return objects and new index.\"\"\"\n",
    "    objects = [tagged[start_idx][0]]\n",
    "    i = start_idx + 1\n",
    "    \n",
    "    while i < len(tagged) and is_object_component(tagged[i][1], tagged[i][0]):\n",
    "        objects.append(tagged[i][0])\n",
    "        i += 1\n",
    "    \n",
    "    return objects, i - 1\n",
    "\n",
    "def process_text(text: str, tokenizer, model, device) -> List[Tuple[str, str]]:\n",
    "    try:\n",
    "        # Initialize tools lazily (only when needed)\n",
    "        spell = SpellChecker()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        # Preprocess text\n",
    "        text = preprocess_text(text)\n",
    "        \n",
    "        # Process text\n",
    "        tokens = word_tokenize(text)\n",
    "        tagged = pos_tag(tokens)\n",
    "        \n",
    "        verb_object_pairs = []\n",
    "        context = ProcessingContext()\n",
    "        \n",
    "        def add_pair():\n",
    "            if context.current_verb and context.current_objects:\n",
    "                lemmatized_verb = lemmatizer.lemmatize(context.current_verb, 'v')\n",
    "                verb_object_pairs.append((lemmatized_verb, ' '.join(context.current_objects)))\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(tagged):\n",
    "            word, tag = tagged[i]\n",
    "            \n",
    "            if is_potential_verb(word, tag):\n",
    "                # Save previous pair if exists\n",
    "                add_pair()\n",
    "                context.current_verb = spell.correction(word)\n",
    "                context.current_objects.clear()\n",
    "                \n",
    "            elif is_object_component(tag, word):\n",
    "                if context.current_verb:\n",
    "                    temp_objects, new_idx = process_compound_objects(tagged, i)\n",
    "                    if any(tagged[j][1].startswith('NN') for j in range(i, new_idx + 1)):\n",
    "                        context.current_objects.extend(temp_objects)\n",
    "                    i = new_idx\n",
    "                    \n",
    "            elif word == ',':\n",
    "                add_pair()\n",
    "                context.reset()\n",
    "            \n",
    "            i += 1\n",
    "        \n",
    "        # Add final pair if exists\n",
    "        add_pair()\n",
    "        \n",
    "        return verb_object_pairs\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"Error processing text: {str(e)}\")\n",
    "        print(f\"Traceback: {traceback.format_exc()}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_task(verb,obj):\n",
    "    \"\"\"Categorize a task based on its verb into one of four categories: Inspection, Cleaning, Maintenance, or Other.\"\"\"\n",
    "    inspection_verbs = {'inspect', 'check', 'examine', 'monitor', 'observe', 'verify', 'test', 'diagnose', 'assess'}\n",
    "    cleaning_verbs = {'clean', 'brush', 'wash', 'wipe', 'descale', 'sanitize', 'flush', 'clear'}\n",
    "    maintenance_verbs = {'lubricate', 'adjust', 'replace', 'repair', 'calibrate', 'install', 'fix', 'tighten', 'service'}\n",
    "    \n",
    "    verb = verb.lower()\n",
    "    if verb in inspection_verbs:\n",
    "        return \"Inspection\"\n",
    "    elif verb in cleaning_verbs:\n",
    "        return \"Cleaning\"\n",
    "    elif verb in maintenance_verbs:\n",
    "        return \"Maintenance\"\n",
    "    else:\n",
    "        return \"Other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_root_cause(verb, obj, text):\n",
    "    \"\"\"Analyze the root cause of an issue based on the maintenance action and context.\"\"\"\n",
    "    # Define component categories and their related terms\n",
    "    components = {\n",
    "        'ice_making': {'ice', 'cube', 'evaporator', 'freezing', 'harvest'},\n",
    "        'refrigeration': {'refrigerant', 'compressor', 'condenser', 'cooling', 'freon', 'leak'},\n",
    "        'water_system': {'water', 'filter', 'pump', 'valve', 'flow', 'drain'},\n",
    "        'electrical': {'power', 'circuit', 'electrical', 'switch', 'sensor', 'board'},\n",
    "        'mechanical': {'belt', 'motor', 'bearing', 'gear', 'pulley', 'fan'}\n",
    "    }\n",
    "    \n",
    "    # Define problem indicators\n",
    "    problem_indicators = {\n",
    "        'failure': {'failed', 'broken', 'not working', 'stopped', 'dead'},\n",
    "        'performance': {'slow', 'weak', 'inefficient', 'poor', 'reduced'},\n",
    "        'noise': {'loud', 'noisy', 'vibration', 'rattling', 'squealing'},\n",
    "        'leakage': {'leak', 'dripping', 'overflow', 'spill'},\n",
    "        'quality': {'dirty', 'contaminated', 'scale', 'buildup', 'poor quality'}\n",
    "    }\n",
    "    \n",
    "    def find_matches(text, term_dict):\n",
    "        text_lower = text.lower()\n",
    "        matches = []\n",
    "        for category, terms in term_dict.items():\n",
    "            if any(term in text_lower for term in terms):\n",
    "                matches.append(category)\n",
    "        return matches\n",
    "    \n",
    "    # Analyze components involved\n",
    "    affected_components = find_matches(text, components)\n",
    "    \n",
    "    # Analyze problem types\n",
    "    problem_types = find_matches(text, problem_indicators)\n",
    "    \n",
    "    # Determine maintenance type\n",
    "    if verb.lower() in {'replace', 'install', 'rebuild'}:\n",
    "        maintenance_type = 'replacement'\n",
    "    elif verb.lower() in {'clean', 'flush', 'sanitize'}:\n",
    "        maintenance_type = 'cleaning'\n",
    "    elif verb.lower() in {'adjust', 'calibrate', 'tune'}:\n",
    "        maintenance_type = 'adjustment'\n",
    "    elif verb.lower() in {'repair', 'fix', 'patch'}:\n",
    "        maintenance_type = 'repair'\n",
    "    else:\n",
    "        maintenance_type = 'other'\n",
    "    \n",
    "    return {\n",
    "        'components': affected_components,\n",
    "        'problem_types': problem_types,\n",
    "        'maintenance_type': maintenance_type,\n",
    "        'action': f\"{verb} {obj}\"\n",
    "    }\n",
    "\n",
    "def get_component_statistics(maintenance_records):\n",
    "    \"\"\"Generate statistics about component issues and maintenance patterns.\"\"\"\n",
    "    component_stats = {\n",
    "        'component_failures': Counter(),\n",
    "        'problem_types': Counter(),\n",
    "        'maintenance_types': Counter(),\n",
    "        'recurring_issues': defaultdict(list)\n",
    "    }\n",
    "    \n",
    "    for record in maintenance_records:\n",
    "        analysis = analyze_root_cause(record['verb'], record['object'], record['text'])\n",
    "        \n",
    "        # Count component failures\n",
    "        for component in analysis['components']:\n",
    "            component_stats['component_failures'][component] += 1\n",
    "        \n",
    "        # Count problem types\n",
    "        for problem in analysis['problem_types']:\n",
    "            component_stats['problem_types'][problem] += 1\n",
    "            \n",
    "        # Count maintenance types\n",
    "        component_stats['maintenance_types'][analysis['maintenance_type']] += 1\n",
    "        \n",
    "        # Track recurring issues\n",
    "        if analysis['components'] and analysis['problem_types']:\n",
    "            issue_key = f\"{','.join(analysis['components'])}:{','.join(analysis['problem_types'])}\"\n",
    "            component_stats['recurring_issues'][issue_key].append(record['date'])\n",
    "    \n",
    "    return component_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all records in df_sample['combined_column']\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(\"Starting to process maintenance records...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize counters and storage\n",
    "all_verb_object_pairs = []  # Keep as list to track frequencies\n",
    "task_frequencies = Counter()\n",
    "maintenance_records = []\n",
    "\n",
    "# Process each record\n",
    "records = df_sample['combined_column'].dropna()\n",
    "total_records = len(records)\n",
    "print(f\"Total records to process: {total_records:,}\")\n",
    "\n",
    "# Initialize progress bar with additional metrics\n",
    "pbar = tqdm(records.items(), \n",
    "            total=total_records,\n",
    "            desc=\"Processing records\",\n",
    "            unit=\"records\",\n",
    "            bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]\")\n",
    "\n",
    "processed_pairs = 0\n",
    "for date_idx, text in pbar:\n",
    "    pairs = process_text(text, tokenizer, model, device)\n",
    "    if pairs:  # Only add if we got valid pairs\n",
    "        for pair in pairs:\n",
    "            verb, obj = pair\n",
    "            task_frequencies[tuple(pair)] += 1\n",
    "            all_verb_object_pairs.append(tuple(pair))\n",
    "            processed_pairs += 1\n",
    "            \n",
    "            # Add record for root cause analysis\n",
    "            maintenance_records.append({\n",
    "                'verb': verb,\n",
    "                'object': obj,\n",
    "                'text': text,\n",
    "                'date': date_idx  # Now using the actual index from items()\n",
    "            })\n",
    "    \n",
    "    # Update progress bar description with additional metrics\n",
    "    if processed_pairs % 100 == 0:  # Update every 100 records to avoid overhead\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = processed_pairs / elapsed if elapsed > 0 else 0\n",
    "        remaining_pairs = total_records - processed_pairs\n",
    "        eta = remaining_pairs / rate if rate > 0 else 0\n",
    "        eta_str = str(timedelta(seconds=int(eta)))\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'Pairs Found': processed_pairs,\n",
    "            'Avg Pairs/Record': f\"{processed_pairs/len(records):.2f}\",\n",
    "            'ETA': eta_str\n",
    "        })\n",
    "\n",
    "end_time = time.time()\n",
    "processing_time = end_time - start_time\n",
    "\n",
    "print(f\"\\nProcessing completed in {timedelta(seconds=int(processing_time))}\")\n",
    "print(f\"Total task occurrences: {len(all_verb_object_pairs):,}\")\n",
    "print(f\"Unique tasks: {len(task_frequencies):,}\")\n",
    "print(f\"Average processing speed: {total_records/processing_time:.1f} records/second\")\n",
    "print(f\"Average pairs per record: {len(all_verb_object_pairs)/total_records:.2f}\")\n",
    "\n",
    "# Group tasks by category with frequencies\n",
    "tasks_by_category = {}\n",
    "category_totals = {}\n",
    "\n",
    "for (verb, obj), freq in task_frequencies.most_common():\n",
    "    category = categorize_task(verb, obj)\n",
    "    if category not in tasks_by_category:\n",
    "        tasks_by_category[category] = []\n",
    "        category_totals[category] = 0\n",
    "    tasks_by_category[category].append((verb, obj, freq))\n",
    "    category_totals[category] += freq\n",
    "\n",
    "# Calculate total tasks\n",
    "total_tasks = sum(category_totals.values())\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_data = {\n",
    "    'Category': [],\n",
    "    'Total_Tasks': [],\n",
    "    'Unique_Tasks': [],\n",
    "    'Percentage': []\n",
    "}\n",
    "\n",
    "for category in sorted(category_totals.keys()):\n",
    "    summary_data['Category'].append(category)\n",
    "    summary_data['Total_Tasks'].append(category_totals[category])\n",
    "    summary_data['Unique_Tasks'].append(len(tasks_by_category[category]))\n",
    "    summary_data['Percentage'].append((category_totals[category] / total_tasks) * 100)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nSummary DataFrame:\")\n",
    "print(summary_df)\n",
    "\n",
    "# After processing all records, analyze root causes\n",
    "print(\"\\nAnalyzing root causes...\")\n",
    "component_stats = get_component_statistics(maintenance_records)\n",
    "\n",
    "# Print root cause analysis results\n",
    "print(\"\\nComponent Failure Analysis:\")\n",
    "for component, count in component_stats['component_failures'].most_common():\n",
    "    print(f\"{component}: {count:,} occurrences\")\n",
    "\n",
    "print(\"\\nProblem Type Analysis:\")\n",
    "for problem, count in component_stats['problem_types'].most_common():\n",
    "    print(f\"{problem}: {count:,} occurrences\")\n",
    "\n",
    "print(\"\\nMaintenance Type Distribution:\")\n",
    "for mtype, count in component_stats['maintenance_types'].most_common():\n",
    "    print(f\"{mtype}: {count:,} occurrences\")\n",
    "\n",
    "print(\"\\nTop Recurring Issues:\")\n",
    "recurring_sorted = sorted(\n",
    "    component_stats['recurring_issues'].items(), \n",
    "    key=lambda x: len(x[1]), \n",
    "    reverse=True\n",
    ")\n",
    "for issue, dates in recurring_sorted[:10]:\n",
    "    components, problems = issue.split(':')\n",
    "    print(f\"Components: {components}\")\n",
    "    print(f\"Problems: {problems}\")\n",
    "    print(f\"Occurrences: {len(dates):,}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs('data/maintenance_analysis', exist_ok=True)\n",
    "\n",
    "# Set pandas display options to prevent wrapping and ensure consistent formatting\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:,}')\n",
    "\n",
    "# Create DataFrames for each category\n",
    "category_dfs = {}\n",
    "for category in ['Maintenance', 'Inspection', 'Cleaning', 'Other']:\n",
    "    if category in tasks_by_category:\n",
    "        category_tasks = []\n",
    "        for verb, obj, freq in tasks_by_category[category]:\n",
    "            # Get root cause info\n",
    "            maintenance_record = next((r for r in maintenance_records if r['verb'] == verb and r['object'] == obj), None)\n",
    "            if maintenance_record:\n",
    "                analysis = analyze_root_cause(verb, obj, maintenance_record['text'])\n",
    "                category_tasks.append({\n",
    "                    'Verb': verb,\n",
    "                    'Object': obj,\n",
    "                    'Frequency': freq,\n",
    "                    'Task Description': f\"{verb} {obj}\".strip(),\n",
    "                    'Components': ', '.join(analysis['components']) if analysis['components'] else 'Other',\n",
    "                    'Problem Types': ', '.join(analysis['problem_types']) if analysis['problem_types'] else 'Unknown',\n",
    "                    'Maintenance Type': analysis['maintenance_type']\n",
    "                })\n",
    "    \n",
    "        if category_tasks:\n",
    "            category_dfs[category] = pd.DataFrame(category_tasks)\n",
    "            category_dfs[category] = category_dfs[category].sort_values('Frequency', ascending=False)\n",
    "            \n",
    "            print(f\"\\n{category} Tasks:\")\n",
    "            print(\"=\" * 50)\n",
    "            \n",
    "            # Format the DataFrame display\n",
    "            df_display = category_dfs[category].head(10).copy()\n",
    "            df_display.index = range(1, len(df_display) + 1)\n",
    "            print(df_display.to_string(\n",
    "                justify='left',\n",
    "                col_space={\n",
    "                    'Verb': 15,\n",
    "                    'Object': 30,\n",
    "                    'Frequency': 12,\n",
    "                    'Task Description': 35,\n",
    "                    'Components': 20,\n",
    "                    'Problem Types': 25,\n",
    "                    'Maintenance Type': 15\n",
    "                }\n",
    "            ))\n",
    "\n",
    "# Save analysis results\n",
    "save_dir = 'data/maintenance_analysis'\n",
    "\n",
    "# Save category DataFrames\n",
    "for category, df in category_dfs.items():\n",
    "    if not df.empty:\n",
    "        filename = os.path.join(save_dir, f\"{category.lower()}_tasks.csv\")\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"\\nSaved {category} tasks to {filename}\")\n",
    "\n",
    "# Save component analysis\n",
    "component_df = pd.DataFrame({\n",
    "    'Component': list(component_stats['component_failures'].keys()),\n",
    "    'Failures': list(component_stats['component_failures'].values())\n",
    "}).sort_values('Failures', ascending=False)\n",
    "\n",
    "filename = os.path.join(save_dir, 'component_analysis.csv')\n",
    "component_df.to_csv(filename, index=False)\n",
    "print(f\"\\nSaved component analysis to {filename}\")\n",
    "\n",
    "# Save problem type analysis\n",
    "problem_df = pd.DataFrame({\n",
    "    'Problem Type': list(component_stats['problem_types'].keys()),\n",
    "    'Occurrences': list(component_stats['problem_types'].values())\n",
    "}).sort_values('Occurrences', ascending=False)\n",
    "\n",
    "filename = os.path.join(save_dir, 'problem_analysis.csv')\n",
    "problem_df.to_csv(filename, index=False)\n",
    "print(f\"\\nSaved problem type analysis to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary of all tasks\n",
    "print(f\"\\nMaintenance Tasks Analysis Summary ({pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')})\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# Create a combined DataFrame of all tasks\n",
    "all_tasks = []\n",
    "for category, df in category_dfs.items():\n",
    "    df = df.copy()\n",
    "    df['Category'] = category\n",
    "    all_tasks.append(df)\n",
    "all_tasks_df = pd.concat(all_tasks, ignore_index=True)\n",
    "\n",
    "# Create task summary DataFrame with formatting\n",
    "summary_df = all_tasks_df.groupby('Category').agg({\n",
    "    'Task Description': 'count',\n",
    "    'Frequency': 'sum'\n",
    "}).rename(columns={\n",
    "    'Task Description': 'Unique Tasks',\n",
    "    'Frequency': 'Total Occurrences'\n",
    "})\n",
    "\n",
    "# Add percentage columns for tasks\n",
    "total_tasks = summary_df['Unique Tasks'].sum()\n",
    "total_occurrences = summary_df['Total Occurrences'].sum()\n",
    "summary_df['% of Tasks'] = (summary_df['Unique Tasks'] / total_tasks * 100).round(1)\n",
    "summary_df['% of Occurrences'] = (summary_df['Total Occurrences'] / total_occurrences * 100).round(1)\n",
    "\n",
    "# Component analysis summary\n",
    "component_summary = pd.DataFrame.from_dict(\n",
    "    component_stats['component_failures'], \n",
    "    orient='index',\n",
    "    columns=['Failures']\n",
    ").sort_values('Failures', ascending=False)\n",
    "component_summary['% of Failures'] = (component_summary['Failures'] / component_summary['Failures'].sum() * 100).round(1)\n",
    "\n",
    "# Problem type analysis summary\n",
    "problem_summary = pd.DataFrame.from_dict(\n",
    "    component_stats['problem_types'], \n",
    "    orient='index',\n",
    "    columns=['Occurrences']\n",
    ").sort_values('Occurrences', ascending=False)\n",
    "problem_summary['% of Problems'] = (problem_summary['Occurrences'] / problem_summary['Occurrences'].sum() * 100).round(1)\n",
    "\n",
    "# Maintenance type distribution\n",
    "maintenance_types = all_tasks_df['Maintenance Type'].value_counts()\n",
    "maintenance_summary = pd.DataFrame({\n",
    "    'Count': maintenance_types,\n",
    "    '% of Total': (maintenance_types / len(all_tasks_df) * 100).round(1)\n",
    "})\n",
    "\n",
    "# Format numbers with thousands separator\n",
    "summary_df['Unique Tasks'] = summary_df['Unique Tasks'].apply(lambda x: f\"{x:,}\")\n",
    "summary_df['Total Occurrences'] = summary_df['Total Occurrences'].apply(lambda x: f\"{x:,}\")\n",
    "\n",
    "# Sort categories in logical order\n",
    "category_order = ['Maintenance', 'Inspection', 'Cleaning', 'Other']\n",
    "summary_df = summary_df.reindex(category_order)\n",
    "\n",
    "# Set display options for better formatting\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:,.0f}' if x.is_integer() else f'{x:.1f}%')\n",
    "\n",
    "# Print all summaries\n",
    "print(\"\\nTask Category Summary:\")\n",
    "print(\"-\" * 80)\n",
    "print(summary_df)\n",
    "print(\"\\nComponent Failure Summary (Top 5):\")\n",
    "print(\"-\" * 80)\n",
    "print(component_summary.head())\n",
    "print(\"\\nProblem Type Summary (Top 5):\")\n",
    "print(\"-\" * 80)\n",
    "print(problem_summary.head())\n",
    "print(\"\\nMaintenance Type Distribution:\")\n",
    "print(\"-\" * 80)\n",
    "print(maintenance_summary)\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total Unique Tasks: {total_tasks:,.0f}\")\n",
    "print(f\"Total Task Occurrences: {total_occurrences:,.0f}\")\n",
    "print(f\"Total Components Analyzed: {len(component_summary):,.0f}\")\n",
    "print(f\"Total Problem Types Identified: {len(problem_summary):,.0f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames for root cause analysis\n",
    "print(\"Detailed Root Cause Analysis\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# 1. Component Failures by Subsystem\n",
    "component_df = pd.DataFrame.from_dict(\n",
    "    component_stats['component_failures'], \n",
    "    orient='index',\n",
    "    columns=['Count']\n",
    ").sort_values('Count', ascending=False)\n",
    "\n",
    "# Group components by subsystem\n",
    "subsystem_mapping = {\n",
    "    'freeze_plate_system': 'Ice Making',\n",
    "    'ice_formation_control': 'Ice Making',\n",
    "    'harvest_system': 'Ice Making',\n",
    "    'ice_handling': 'Ice Making',\n",
    "    'water_distribution_ice': 'Ice Making',\n",
    "    'compressor': 'Refrigeration',\n",
    "    'condenser': 'Refrigeration',\n",
    "    'refrigerant_circuit': 'Refrigeration',\n",
    "    'expansion_device': 'Refrigeration',\n",
    "    'water_supply': 'Water System',\n",
    "    'water_pump': 'Water System',\n",
    "    'water_distribution': 'Water System',\n",
    "    'drain_system': 'Water System',\n",
    "    'control_board': 'Electrical',\n",
    "    'sensors': 'Electrical',\n",
    "    'electrical_components': 'Electrical',\n",
    "    'wiring': 'Electrical',\n",
    "    'motors': 'Mechanical',\n",
    "    'belts_chains': 'Mechanical',\n",
    "    'framework': 'Mechanical'\n",
    "}\n",
    "\n",
    "component_df['Subsystem'] = component_df.index.map(lambda x: subsystem_mapping.get(x, 'Other'))\n",
    "component_df['Percentage'] = (component_df['Count'] / component_df['Count'].sum() * 100).round(1)\n",
    "\n",
    "# Group by subsystem for summary\n",
    "subsystem_summary = component_df.groupby('Subsystem').agg({\n",
    "    'Count': 'sum',\n",
    "    'Percentage': 'sum'\n",
    "}).sort_values('Count', ascending=False)\n",
    "\n",
    "print(\"\\nFailures by Subsystem:\")\n",
    "print(\"-\" * 100)\n",
    "print(subsystem_summary.to_string(float_format=lambda x: f'{x:,.1f}'))\n",
    "\n",
    "print(\"\\nTop 10 Specific Component Failures:\")\n",
    "print(\"-\" * 100)\n",
    "print(component_df.head(10).to_string(float_format=lambda x: f'{x:,.1f}'))\n",
    "\n",
    "# 2. Problem Types with Severity\n",
    "problem_df = pd.DataFrame.from_dict(\n",
    "    component_stats['problem_types'], \n",
    "    orient='index',\n",
    "    columns=['Count']\n",
    ").sort_values('Count', ascending=False)\n",
    "\n",
    "problem_df['Percentage'] = (problem_df['Count'] / problem_df['Count'].sum() * 100).round(1)\n",
    "\n",
    "print(\"\\nProblem Types by Frequency:\")\n",
    "print(\"-\" * 100)\n",
    "print(problem_df.to_string(float_format=lambda x: f'{x:,.1f}'))\n",
    "\n",
    "# 3. Recurring Issues Analysis\n",
    "recurring_issues = []\n",
    "for issue, dates in component_stats['recurring_issues'].items():\n",
    "    components, problems = issue.split(':')\n",
    "    \n",
    "    # Convert dates to datetime if they aren't already\n",
    "    date_objects = []\n",
    "    for date in dates:\n",
    "        if isinstance(date, (int, float)):\n",
    "            # If date is a timestamp\n",
    "            date_obj = pd.to_datetime(date, unit='s')\n",
    "        elif isinstance(date, str):\n",
    "            # If date is a string\n",
    "            date_obj = pd.to_datetime(date)\n",
    "        else:\n",
    "            # If date is already datetime\n",
    "            date_obj = date\n",
    "        date_objects.append(date_obj)\n",
    "    \n",
    "    if date_objects:\n",
    "        mtbf_days = (max(date_objects) - min(date_objects)).total_seconds() / (86400 * len(date_objects))\n",
    "        last_occurrence = max(date_objects).strftime('%Y-%m-%d')\n",
    "    else:\n",
    "        mtbf_days = 0\n",
    "        last_occurrence = 'N/A'\n",
    "    \n",
    "    recurring_issues.append({\n",
    "        'Components': components,\n",
    "        'Problems': problems,\n",
    "        'Occurrences': len(dates),\n",
    "        'Mean Time Between Failures': f\"{mtbf_days:.1f} days\",\n",
    "        'Last Occurrence': last_occurrence\n",
    "    })\n",
    "\n",
    "recurring_df = pd.DataFrame(recurring_issues)\n",
    "if not recurring_df.empty:\n",
    "    recurring_df = recurring_df.sort_values('Occurrences', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop Recurring Issues (with MTBF):\")\n",
    "    print(\"-\" * 100)\n",
    "    print(recurring_df.head(10).to_string(index=False))\n",
    "\n",
    "# Save detailed analysis to CSV\n",
    "output_dir = 'data/maintenance_analysis'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "component_df.to_csv(f'{output_dir}/component_failures_detailed.csv')\n",
    "subsystem_summary.to_csv(f'{output_dir}/subsystem_summary.csv')\n",
    "problem_df.to_csv(f'{output_dir}/problem_types_detailed.csv')\n",
    "if not recurring_df.empty:\n",
    "    recurring_df.to_csv(f'{output_dir}/recurring_issues_detailed.csv', index=False)\n",
    "\n",
    "print(f\"\\nDetailed analysis files saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_root_cause(verb, obj, text):\n",
    "    \"\"\"Analyze the root cause of an issue based on the maintenance action and context.\"\"\"\n",
    "    # Define detailed component categories and their related terms\n",
    "    components = {\n",
    "        # Ice Making System\n",
    "        'freeze_plate_system': {\n",
    "            'evaporator plate', 'freeze plate', 'cooling surface', 'grid plate', \n",
    "            'ice mold', 'cube form', 'ice formation', 'freezing surface',\n",
    "            'water distribution plate', 'cell size', 'bridge thickness'\n",
    "        },\n",
    "        'ice_formation_control': {\n",
    "            'thickness sensor', 'ice thickness', 'bridge control',\n",
    "            'water level sensor', 'formation time', 'freeze cycle',\n",
    "            'water curtain', 'spray time', 'freeze timer'\n",
    "        },\n",
    "        'harvest_system': {\n",
    "            'harvest valve', 'hot gas valve', 'defrost valve', \n",
    "            'harvest timer', 'harvest assist', 'release mechanism',\n",
    "            'hot gas bypass', 'harvest pressure', 'harvest solenoid',\n",
    "            'harvest check valve', 'harvest complete switch'\n",
    "        },\n",
    "        'ice_handling': {\n",
    "            'ice bin', 'storage bin', 'bin thermostat', 'bin control',\n",
    "            'ice chute', 'dispenser', 'agitator', 'auger motor',\n",
    "            'ice gate', 'bin door', 'bin level sensor', 'ice shield'\n",
    "        },\n",
    "        'water_distribution_ice': {\n",
    "            'water pump', 'spray bar', 'spray nozzles', 'water curtain',\n",
    "            'water trough', 'water sump', 'distribution tube',\n",
    "            'water level', 'float valve', 'reservoir'\n",
    "        },\n",
    "        \n",
    "        # Refrigeration System\n",
    "        'compressor_system': {\n",
    "            'compressor', 'piston', 'scroll', 'reed valve', 'valve plate',\n",
    "            'discharge line', 'suction line', 'oil level', 'crankcase',\n",
    "            'compressor motor', 'windings', 'terminal', 'overload protector'\n",
    "        },\n",
    "        'condenser_system': {\n",
    "            'condenser coil', 'fan motor', 'fan blade', 'air flow',\n",
    "            'head pressure', 'subcooling', 'fins', 'coil surface',\n",
    "            'condenser pressure', 'air cooled', 'water cooled'\n",
    "        },\n",
    "        'refrigerant_circuit': {\n",
    "            'refrigerant', 'freon', 'charge', 'leak', 'pressure',\n",
    "            'filter drier', 'sight glass', 'accumulator', 'receiver',\n",
    "            'liquid line', 'suction line', 'discharge line'\n",
    "        },\n",
    "        'expansion_system': {\n",
    "            'TXV', 'expansion valve', 'capillary tube', 'orifice',\n",
    "            'superheat', 'bulb', 'equalizer', 'distributor',\n",
    "            'metering device', 'restrictor'\n",
    "        },\n",
    "        \n",
    "        # Water System\n",
    "        'water_supply': {\n",
    "            'water line', 'inlet valve', 'water pressure', 'filter',\n",
    "            'strainer', 'softener', 'water quality', 'supply pipe',\n",
    "            'shut off valve', 'pressure regulator'\n",
    "        },\n",
    "        'water_pump_system': {\n",
    "            'circulation pump', 'impeller', 'seal', 'pump motor',\n",
    "            'pump capacity', 'pump housing', 'bearing', 'shaft',\n",
    "            'pump pressure', 'pump strainer'\n",
    "        },\n",
    "        'water_distribution': {\n",
    "            'spray nozzles', 'water curtain', 'distributor', 'tube',\n",
    "            'spray pattern', 'distribution manifold', 'spray bar',\n",
    "            'water flow', 'distribution uniformity'\n",
    "        },\n",
    "        'drain_system': {\n",
    "            'drain line', 'condensate', 'drain pan', 'float switch',\n",
    "            'pump out', 'drain valve', 'trap', 'vent', 'slope',\n",
    "            'drain heater', 'overflow protection'\n",
    "        },\n",
    "        \n",
    "        # Electrical System\n",
    "        'control_board': {\n",
    "            'PCB', 'control board', 'controller', 'motherboard',\n",
    "            'processor', 'relay board', 'display board', 'interface',\n",
    "            'memory', 'firmware', 'programming'\n",
    "        },\n",
    "        'sensors_system': {\n",
    "            'thermistor', 'probe', 'sensor', 'thermostat', 'float switch',\n",
    "            'bin level sensor', 'pressure sensor', 'temperature sensor',\n",
    "            'water level sensor', 'ice thickness sensor'\n",
    "        },\n",
    "        'electrical_components': {\n",
    "            'contactor', 'relay', 'capacitor', 'transformer', 'fuse',\n",
    "            'overload', 'terminal block', 'wire connector', 'breaker',\n",
    "            'power supply', 'voltage regulator'\n",
    "        },\n",
    "        'wiring_system': {\n",
    "            'wire harness', 'connection', 'terminal', 'plug', 'socket',\n",
    "            'ground wire', 'power cable', 'communication wire',\n",
    "            'insulation', 'conduit', 'junction box'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Define specific problem types with detailed indicators\n",
    "    problem_indicators = {\n",
    "        'mechanical_failure': {\n",
    "            'seized', 'stuck', 'broken', 'cracked', 'worn', 'misaligned',\n",
    "            'loose', 'bent', 'damaged', 'jammed', 'binding', 'stripped'\n",
    "        },\n",
    "        'electrical_failure': {\n",
    "            'short', 'open circuit', 'voltage', 'tripped', 'burnt',\n",
    "            'no power', 'grounded', 'overload', 'electrical noise',\n",
    "            'intermittent power', 'voltage drop'\n",
    "        },\n",
    "        'ice_quality_issue': {\n",
    "            'cloudy ice', 'small cubes', 'incomplete cubes', 'malformed',\n",
    "            'hollow cubes', 'white spots', 'bridging', 'long harvest',\n",
    "            'slow production', 'uneven size'\n",
    "        },\n",
    "        'performance_issue': {\n",
    "            'slow', 'inefficient', 'reduced', 'low production',\n",
    "            'poor quality', 'inconsistent', 'erratic', 'unstable',\n",
    "            'degraded', 'below spec'\n",
    "        },\n",
    "        'leakage_issue': {\n",
    "            'leak', 'drip', 'overflow', 'flood', 'seal failure',\n",
    "            'gasket leak', 'water loss', 'refrigerant leak',\n",
    "            'oil leak', 'seepage'\n",
    "        },\n",
    "        'contamination': {\n",
    "            'dirty', 'scale', 'calcium', 'mineral', 'debris', 'buildup',\n",
    "            'corrosion', 'rust', 'slime', 'mold', 'algae', 'sediment'\n",
    "        },\n",
    "        'noise_vibration': {\n",
    "            'noise', 'vibration', 'rattle', 'squeal', 'bang', 'hum',\n",
    "            'grinding', 'knocking', 'whistling', 'clicking', 'rumbling'\n",
    "        },\n",
    "        'control_system': {\n",
    "            'error code', 'not responding', 'incorrect', 'erratic',\n",
    "            'intermittent', 'display error', 'sensor error', 'communication error',\n",
    "            'program error', 'calibration error'\n",
    "        },\n",
    "        'temperature_issue': {\n",
    "            'hot', 'cold', 'warm', 'freezing', 'not cooling',\n",
    "            'high temp', 'low temp', 'inconsistent temp',\n",
    "            'temperature swing', 'poor temperature control'\n",
    "        },\n",
    "        'pressure_issue': {\n",
    "            'high pressure', 'low pressure', 'no pressure', 'pressure drop',\n",
    "            'pressure fluctuation', 'excessive pressure', 'insufficient pressure',\n",
    "            'pressure lockout', 'pressure control'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Define maintenance types with detailed activities\n",
    "    maintenance_types = {\n",
    "        'preventive': {\n",
    "            'inspect', 'clean', 'adjust', 'lubricate', 'tighten',\n",
    "            'calibrate', 'test', 'check', 'measure', 'scheduled',\n",
    "            'routine', 'preventative'\n",
    "        },\n",
    "        'corrective': {\n",
    "            'repair', 'replace', 'fix', 'rebuild', 'overhaul',\n",
    "            'restore', 'rectify', 'correct', 'resolve', 'service'\n",
    "        },\n",
    "        'predictive': {\n",
    "            'monitor', 'analyze', 'trend', 'forecast', 'predict',\n",
    "            'assess', 'evaluate', 'diagnose', 'investigate', 'study'\n",
    "        },\n",
    "        'emergency': {\n",
    "            'breakdown', 'failure', 'emergency', 'urgent', 'critical',\n",
    "            'immediate', 'unplanned', 'unexpected', 'sudden', 'acute'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def find_matches(text, term_dict):\n",
    "        \"\"\"Find matches in text for terms in term_dict.\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        matches = []\n",
    "        for category, terms in term_dict.items():\n",
    "            if any(term.lower() in text_lower for term in terms):\n",
    "                matches.append(category)\n",
    "        return matches\n",
    "    \n",
    "    # Combine all text fields for analysis\n",
    "    full_text = f\"{text} {obj} {verb}\".lower()\n",
    "    \n",
    "    # Find all matches\n",
    "    found_components = find_matches(full_text, components)\n",
    "    found_problems = find_matches(full_text, problem_indicators)\n",
    "    \n",
    "    # Determine maintenance type\n",
    "    maint_matches = find_matches(full_text, maintenance_types)\n",
    "    maintenance_type = maint_matches[0] if maint_matches else 'unspecified'\n",
    "    \n",
    "    # Determine severity based on various factors\n",
    "    severity_indicators = {\n",
    "        'high': {'emergency', 'critical', 'urgent', 'failure', 'breakdown', 'safety',\n",
    "                'immediate', 'severe', 'major', 'serious'},\n",
    "        'medium': {'degraded', 'warning', 'attention', 'minor failure', 'reduced',\n",
    "                  'inconsistent', 'poor', 'issue'},\n",
    "        'low': {'routine', 'normal', 'scheduled', 'preventive', 'adjust', 'check',\n",
    "                'inspect', 'clean', 'monitor'}\n",
    "    }\n",
    "    \n",
    "    # Determine severity\n",
    "    severity = 'normal'\n",
    "    for level, indicators in severity_indicators.items():\n",
    "        if any(indicator in full_text for indicator in indicators):\n",
    "            severity = level\n",
    "            break\n",
    "    \n",
    "    return {\n",
    "        'components': found_components,\n",
    "        'problem_types': found_problems,\n",
    "        'maintenance_type': maintenance_type,\n",
    "        'severity': severity,\n",
    "        'timestamp': pd.Timestamp.now()  # Add timestamp for tracking\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Perform root cause analysis on maintenance records\n",
    "print(\"Detailed Root Cause Analysis\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Initialize counters for analysis\n",
    "component_failures = defaultdict(int)\n",
    "problem_types = defaultdict(int)\n",
    "maintenance_types = defaultdict(int)\n",
    "severity_counts = defaultdict(int)\n",
    "recurring_issues = defaultdict(list)\n",
    "\n",
    "# Process all records\n",
    "print(f\"\\nAnalyzing {len(maintenance_records)} maintenance records...\")\n",
    "\n",
    "for record in tqdm(maintenance_records, desc=\"Processing records\"):\n",
    "    # Perform root cause analysis\n",
    "    analysis = analyze_root_cause(record['verb'], record['object'], record['text'])\n",
    "    \n",
    "    # Count component failures\n",
    "    for component in analysis['components']:\n",
    "        component_failures[component] += 1\n",
    "    \n",
    "    # Count problem types\n",
    "    for problem in analysis['problem_types']:\n",
    "        problem_types[problem] += 1\n",
    "    \n",
    "    # Count maintenance types\n",
    "    maintenance_types[analysis['maintenance_type']] += 1\n",
    "    \n",
    "    # Count severity levels\n",
    "    severity_counts[analysis['severity']] += 1\n",
    "    \n",
    "    # Track recurring issues with proper date handling\n",
    "    if analysis['components'] and analysis['problem_types']:\n",
    "        components_str = ','.join(sorted(list(analysis['components'])))\n",
    "        problems_str = ','.join(sorted(list(analysis['problem_types'])))\n",
    "        issue_key = f\"{components_str}:{problems_str}\"\n",
    "        # Convert date to pandas Timestamp\n",
    "        date = pd.to_datetime(record['date']) if not isinstance(record['date'], pd.Timestamp) else record['date']\n",
    "        recurring_issues[issue_key].append(date)\n",
    "\n",
    "# Create DataFrames for analysis\n",
    "print(\"\\n1. Component Failures Analysis\")\n",
    "print(\"-\" * 80)\n",
    "component_df = pd.DataFrame.from_dict(\n",
    "    component_failures, \n",
    "    orient='index',\n",
    "    columns=['Count']\n",
    ").sort_values('Count', ascending=False)\n",
    "\n",
    "component_df['Percentage'] = (component_df['Count'] / component_df['Count'].sum() * 100).round(1)\n",
    "\n",
    "# Group by subsystem\n",
    "subsystem_mapping = {\n",
    "    'freeze_plate_system': 'Ice Making',\n",
    "    'ice_formation_control': 'Ice Making',\n",
    "    'harvest_system': 'Ice Making',\n",
    "    'ice_handling': 'Ice Making',\n",
    "    'water_distribution_ice': 'Ice Making',\n",
    "    'compressor_system': 'Refrigeration',\n",
    "    'condenser_system': 'Refrigeration',\n",
    "    'refrigerant_circuit': 'Refrigeration',\n",
    "    'expansion_system': 'Refrigeration',\n",
    "    'water_supply': 'Water System',\n",
    "    'water_pump_system': 'Water System',\n",
    "    'water_distribution': 'Water System',\n",
    "    'drain_system': 'Water System',\n",
    "    'control_board': 'Electrical',\n",
    "    'sensors_system': 'Electrical',\n",
    "    'electrical_components': 'Electrical',\n",
    "    'wiring_system': 'Electrical'\n",
    "}\n",
    "\n",
    "component_df['Subsystem'] = component_df.index.map(lambda x: subsystem_mapping.get(x, 'Other'))\n",
    "subsystem_summary = component_df.groupby('Subsystem').agg({\n",
    "    'Count': 'sum',\n",
    "    'Percentage': 'sum'\n",
    "}).sort_values('Count', ascending=False)\n",
    "\n",
    "print(\"\\nFailures by Subsystem:\")\n",
    "print(subsystem_summary.to_string(float_format=lambda x: f'{x:,.1f}'))\n",
    "\n",
    "print(\"\\nTop 10 Component Failures:\")\n",
    "print(component_df.head(10).to_string(float_format=lambda x: f'{x:,.1f}'))\n",
    "\n",
    "# Problem Types Analysis\n",
    "print(\"\\n2. Problem Types Analysis\")\n",
    "print(\"-\" * 80)\n",
    "problem_df = pd.DataFrame.from_dict(\n",
    "    problem_types,\n",
    "    orient='index',\n",
    "    columns=['Count']\n",
    ").sort_values('Count', ascending=False)\n",
    "\n",
    "problem_df['Percentage'] = (problem_df['Count'] / problem_df['Count'].sum() * 100).round(1)\n",
    "print(\"\\nProblem Type Distribution:\")\n",
    "print(problem_df.to_string(float_format=lambda x: f'{x:,.1f}'))\n",
    "\n",
    "# Maintenance Types Analysis\n",
    "print(\"\\n3. Maintenance Types Analysis\")\n",
    "print(\"-\" * 80)\n",
    "maintenance_df = pd.DataFrame.from_dict(\n",
    "    maintenance_types,\n",
    "    orient='index',\n",
    "    columns=['Count']\n",
    ").sort_values('Count', ascending=False)\n",
    "\n",
    "maintenance_df['Percentage'] = (maintenance_df['Count'] / maintenance_df['Count'].sum() * 100).round(1)\n",
    "print(\"\\nMaintenance Type Distribution:\")\n",
    "print(maintenance_df.to_string(float_format=lambda x: f'{x:,.1f}'))\n",
    "\n",
    "# Severity Analysis\n",
    "print(\"\\n4. Severity Analysis\")\n",
    "print(\"-\" * 80)\n",
    "severity_df = pd.DataFrame.from_dict(\n",
    "    severity_counts,\n",
    "    orient='index',\n",
    "    columns=['Count']\n",
    ").sort_values('Count', ascending=False)\n",
    "\n",
    "severity_df['Percentage'] = (severity_df['Count'] / severity_df['Count'].sum() * 100).round(1)\n",
    "print(\"\\nSeverity Distribution:\")\n",
    "print(severity_df.to_string(float_format=lambda x: f'{x:,.1f}'))\n",
    "\n",
    "# Recurring Issues Analysis\n",
    "print(\"\\n5. Recurring Issues Analysis\")\n",
    "print(\"-\" * 80)\n",
    "recurring_data = []\n",
    "for issue, dates in recurring_issues.items():\n",
    "    if len(dates) > 1:  # Only include if it occurred multiple times\n",
    "        components, problems = issue.split(':')\n",
    "        # Calculate time difference\n",
    "        date_range = pd.date_range(min(dates), max(dates))\n",
    "        mtbf_days = (max(dates) - min(dates)).total_seconds() / (86400 * (len(dates) - 1))\n",
    "        \n",
    "        recurring_data.append({\n",
    "            'Components': components,\n",
    "            'Problems': problems,\n",
    "            'Occurrences': len(dates),\n",
    "            'Mean Time Between Failures': f\"{mtbf_days:.1f} days\",\n",
    "            'First Occurrence': min(dates).strftime('%Y-%m-%d'),\n",
    "            'Last Occurrence': max(dates).strftime('%Y-%m-%d')\n",
    "        })\n",
    "\n",
    "recurring_df = pd.DataFrame(recurring_data)\n",
    "if not recurring_df.empty:\n",
    "    recurring_df = recurring_df.sort_values('Occurrences', ascending=False)\n",
    "    print(\"\\nTop 10 Recurring Issues:\")\n",
    "    print(recurring_df.head(10).to_string(index=False))\n",
    "\n",
    "# Save analysis results\n",
    "output_dir = 'data/maintenance_analysis'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "component_df.to_csv(f'{output_dir}/component_failures.csv')\n",
    "subsystem_summary.to_csv(f'{output_dir}/subsystem_summary.csv')\n",
    "problem_df.to_csv(f'{output_dir}/problem_types.csv')\n",
    "maintenance_df.to_csv(f'{output_dir}/maintenance_types.csv')\n",
    "severity_df.to_csv(f'{output_dir}/severity_analysis.csv')\n",
    "recurring_df.to_csv(f'{output_dir}/recurring_issues.csv', index=False)\n",
    "\n",
    "print(f\"\\nAnalysis files saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coolsys_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
